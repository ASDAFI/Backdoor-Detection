{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4a971c-c0de-459d-85f5-bff69886bb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger('detector')\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "LOGGER.addHandler(stream_handler)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOGGER.info(f'device is {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc66af9a-bb20-436d-8b5f-f1b186279021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ind = None\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        if self.ind is not None:\n",
    "            out += shortcut[:, self.ind, :, :]\n",
    "        else:\n",
    "            out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            BLOCK = block(self.in_planes, planes, stride)\n",
    "            layers.append(BLOCK)\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18(num_classes=10):\n",
    "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b73457-01d1-4e9c-9047-5b46046ba009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "ROOT_DIR = 'eval_dataset'\n",
    "\n",
    "def load_model(num_classes, model_path):\n",
    "    model = PreActResNet18(num_classes)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_test(idx: int):\n",
    "    test_root_dir = os.path.join(ROOT_DIR, str(idx))\n",
    "\n",
    "    metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
    "\n",
    "    num_classes = metadata['num_classes']\n",
    "    ground_truth = metadata['ground_truth']\n",
    "    images_root_dir = metadata['test_images_folder_address']\n",
    "    transformation = metadata['transformation']\n",
    "\n",
    "    model_path = os.path.join(test_root_dir, 'model.pt')\n",
    "\n",
    "    if images_root_dir[0] == '.':\n",
    "        images_root_dir = images_root_dir[2:]\n",
    "\n",
    "    images_root_dir = os.path.join(test_root_dir, images_root_dir)\n",
    "\n",
    "\n",
    "    model = load_model(num_classes, model_path)\n",
    "\n",
    "    return model, num_classes, ground_truth, transformation, images_root_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d21385-b783-49cb-9aff-86188dac6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, num_classes, ground_truth, transformation, images_root_dir = load_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3b32ed-0af6-4f74-a063-1ed7b00c3964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d47f337-487d-4080-9fa4-8b48b5864764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def transform_images(images_path: List[str], transformation: transforms.Compose):\n",
    "    transformed_images = []\n",
    "    for img_path in images_path:\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = transformation(image)\n",
    "            transformed_images.append(image)\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "    if not transformed_images:\n",
    "        LOGGER.error(\"No images were loaded. Please check the images_path list.\")\n",
    "\n",
    "    return torch.stack(transformed_images).to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9f6111-f933-4391-97c7-837c589af3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_normalization_params(transformation: transforms.Compose):\n",
    "    mean = None\n",
    "    std = None\n",
    "\n",
    "    for transform in transformation.transforms:\n",
    "        if isinstance(transform, transforms.Normalize):\n",
    "            mean = transform.mean\n",
    "            std = transform.std\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def get_logits_and_probs(model: PreActResNet, transformed_images: torch.Tensor):\n",
    "    logits = model(transformed_images)\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    return probabilities, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77010de-b95e-4755-b4bf-b53db76d3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calculate_margins(probs: List[torch.Tensor],\n",
    "                      labels: List[int]) -> Tuple[defaultdict[int, List[float]],\n",
    "                                                           defaultdict[int, List[float]]]:\n",
    "    accepted_margins = defaultdict(list)\n",
    "    failed_margins = defaultdict(list)\n",
    "    for i in range(len(probs)):\n",
    "        topk = torch.topk(probs[i], k=2, largest=True, sorted=True)\n",
    "        topk_values = topk.values\n",
    "\n",
    "        margin = topk_values[0].item() - topk_values[1].item()\n",
    "        if labels[i] == torch.argmax(probs[i]).item():\n",
    "            accepted_margins[labels[i]].append(margin)\n",
    "        else:\n",
    "            failed_margins[torch.argmax(probs[i]).item()].append(margin)\n",
    "    return accepted_margins, failed_margins\n",
    "\n",
    "def find_safe_margin(accepted_margins: defaultdict[int, List[float]],\n",
    "                     failed_margins: defaultdict[int, List[float]]):\n",
    "    min_accepted_margins = dict()\n",
    "    for c in accepted_margins.keys():\n",
    "        max_failed = float('inf')\n",
    "        if failed_margins[c]:\n",
    "            max_failed = max(failed_margins[c])\n",
    "\n",
    "        min_accepted_margin = max_failed\n",
    "        for margin in accepted_margins[c]:\n",
    "            if margin <= min_accepted_margin:\n",
    "                min_accepted_margin = margin\n",
    "\n",
    "        min_accepted_margins[c] = min_accepted_margin\n",
    "    return min_accepted_margins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97eec89a-9707-438e-8839-ba8caedccfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_image(optimized_img: torch.Tensor, mean: float, std: float):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1).to(optimized_img.device)\n",
    "    std = torch.tensor(std).view(-1, 1, 1).to(optimized_img.device)\n",
    "\n",
    "    min_val = (0.0 - mean) / std\n",
    "    max_val = (1.0 - mean) / std\n",
    "\n",
    "    optimized_img = torch.clamp(optimized_img, min=min_val, max=max_val)\n",
    "\n",
    "    return optimized_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c90c117-f784-480d-86b8-bb4914a337c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_top_images_per_class(probs: List[torch.Tensor],\n",
    "                                images: List[torch.Tensor],\n",
    "                                labels: List[int],\n",
    "                                num_classes: int, top_k=3) -> defaultdict[int, List[torch.Tensor]]:\n",
    "\n",
    "    selected_images_per_class = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (prob, label) in enumerate(zip(probs, labels)):\n",
    "            detected_class = torch.argmax(prob).item()\n",
    "            if detected_class == label:\n",
    "                confidence = prob[detected_class].item()\n",
    "                selected_images_per_class[label].append((confidence, images[idx]))\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        class_images = selected_images_per_class[c]\n",
    "        sorted_images = sorted(class_images, key=lambda x: x[0], reverse=True)\n",
    "        selected_images_per_class[c] = [img for _, img in sorted_images[:top_k]]\n",
    "    return selected_images_per_class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f1c8717-2ce7-412c-a320-ed4761e6f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_image(transformation):\n",
    "    transformation = transforms.Compose([transforms.ToPILImage()] + list(transformation.transforms))\n",
    "    return transformation(torch.randn(3, *(220, 220)) * 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c56ee2f-4858-45b8-9570-b09be2643d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def compute_max_margin(model: PreActResNet,\n",
    "                       selected_images_per_class: dict[int, List],\n",
    "                       num_classes: int,\n",
    "                       projection_mean: float, projection_std: float,\n",
    "                       max_iterations=1000, lr=0.01, tolerance=1e-5, max_img_per_class=3):\n",
    "    model.eval()\n",
    "    max_margins = {}\n",
    "    all_margins_per_class = defaultdict(list)\n",
    "    all_triggers_per_class = defaultdict(list)\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        margins = []\n",
    "        triggers = []\n",
    "\n",
    "        LOGGER.info(f\"\\nProcessing Class {c}/{num_classes - 1}\")\n",
    "        images_to_optimize = []\n",
    "        for k in range(num_classes):\n",
    "            if k == c:\n",
    "                continue\n",
    "            images = selected_images_per_class.get(k, [])\n",
    "            if not images:\n",
    "                continue\n",
    "            images_to_optimize.extend(images)\n",
    "        random.shuffle(images_to_optimize)\n",
    "        images_to_optimize = images_to_optimize[:max_img_per_class]\n",
    "\n",
    "        LOGGER.info(f\"  Total images to optimize for class {c}: {len(images_to_optimize)}\")\n",
    "\n",
    "        for idx, img in enumerate(images_to_optimize):\n",
    "            max_margin = -float('inf')\n",
    "            trigger = None\n",
    "\n",
    "            optimized_img = img.clone().detach().to(DEVICE)\n",
    "            optimized_img.requires_grad = True\n",
    "\n",
    "            optimizer = torch.optim.Adam([optimized_img], lr=lr)\n",
    "\n",
    "            f_old = None\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                optimizer.zero_grad()\n",
    "                probs, logits = get_logits_and_probs(model, optimized_img.unsqueeze(0))\n",
    "                logits = logits.squeeze(0)\n",
    "                g_c = logits[c]\n",
    "\n",
    "                mask = torch.ones(num_classes, dtype=torch.bool).to(DEVICE)\n",
    "                mask[c] = False\n",
    "                g_k = torch.max(logits[mask])\n",
    "\n",
    "                margin = g_c - g_k\n",
    "                if margin > max_margin:\n",
    "                    max_margin = margin\n",
    "                    trigger = optimized_img.detach().cpu().clone()\n",
    "\n",
    "                loss = -margin\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    optimized_img.copy_(project_image(optimized_img, projection_mean, projection_std))\n",
    "\n",
    "                f_new = margin.item()\n",
    "                if f_old is not None:\n",
    "                    relative_change = abs(f_new - f_old) / (abs(f_old) + 1e-8)\n",
    "                    if relative_change < tolerance:\n",
    "                        break\n",
    "                f_old = f_new\n",
    "            margins.append(max_margin)\n",
    "            triggers.append(trigger)\n",
    "            \n",
    "\n",
    "        if margins:\n",
    "            all_triggers_per_class[c] = triggers.copy()\n",
    "            all_margins_per_class[c] = margins.copy()\n",
    "            max_margins[c] = max(margins)\n",
    "            LOGGER.info(f\"  Maximum Margin for class {c}: {max_margins[c]:.4f}\")\n",
    "        else:\n",
    "            max_margins[c] = 0.0\n",
    "            LOGGER.info(f\"  No margins computed for class {c}.\")\n",
    "\n",
    "    max_margins = [x[1].item() for x in sorted(max_margins.items(), key=lambda x: x[0])]\n",
    "    \n",
    "    return max_margins, all_triggers_per_class, all_margins_per_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df9eb42-ad95-4b28-86a1-e49024bc4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compute_p_values(gamma_list: List[float],\n",
    "                     distributions=['gamma', 'norm'], p_value_type='standard') -> dict[str, float]:\n",
    "\n",
    "    if not gamma_list:\n",
    "        raise ValueError(\"gamma_list is empty.\")\n",
    "\n",
    "    gamma_array = np.array(gamma_list)\n",
    "    r_max = np.max(gamma_array)\n",
    "    n = len(gamma_array)\n",
    "\n",
    "    null_data = gamma_array[gamma_array != r_max]\n",
    "    if len(null_data) == 0:\n",
    "        raise ValueError(\"All values in gamma_list are identical.\")\n",
    "\n",
    "    p_values = {}\n",
    "\n",
    "    for dist_name in distributions:\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                if dist_name == 'gamma':\n",
    "                    a, loc, scale = stats.gamma.fit(null_data, floc=0)\n",
    "                    fitted_dist = stats.gamma(a, loc=loc, scale=scale)\n",
    "                elif dist_name == 'norm':\n",
    "                    mu, sigma = stats.norm.fit(null_data)\n",
    "                    fitted_dist = stats.norm(loc=mu, scale=sigma)\n",
    "                elif dist_name == 'expon':\n",
    "                    loc, scale = stats.expon.fit(null_data)\n",
    "                    fitted_dist = stats.expon(loc=loc, scale=scale)\n",
    "                elif dist_name == 'beta':\n",
    "                    a, b, loc, scale = stats.beta.fit(null_data, floc=0, fscale=1)\n",
    "                    fitted_dist = stats.beta(a, b, loc=loc, scale=scale)\n",
    "                elif dist_name == 'lognorm':\n",
    "                    s, loc, scale = stats.lognorm.fit(null_data, floc=0)\n",
    "                    fitted_dist = stats.lognorm(s, loc=loc, scale=scale)\n",
    "                else:\n",
    "                    LOGGER.error(f\"Distribution '{dist_name}' is not supported.\")\n",
    "                    continue\n",
    "\n",
    "                H0_r_max = fitted_dist.cdf(r_max)\n",
    "\n",
    "                if p_value_type == 'standard':\n",
    "                    p_val = 1 - H0_r_max**n\n",
    "                elif p_value_type == 'user_specified':\n",
    "                    p_val = H0_r_max**(n-1)\n",
    "                else:\n",
    "                    LOGGER.error(f\"p_value_type '{p_value_type}' is not recognized. Choose 'standard' or 'user_specified'.\")\n",
    "                    continue\n",
    "\n",
    "                p_values[dist_name] = p_val\n",
    "\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"An error occurred while fitting distribution '{dist_name}': {e}\")\n",
    "            p_values[dist_name] = None\n",
    "\n",
    "    return p_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876840b8-6ce2-4847-9bf4-fc9522cb5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, num_classes, ground_truth, transformation, images_root_dir = load_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a37c9138-3c2d-4038-81ff-55c55da50ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f1d4ed-1156-40c5-b3d0-5f67d7a492c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 34.5235\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 45.5479\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 24.9980\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 28.4304\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 27.2063\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 94.8370\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 26.2219\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 36.2613\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 42.0086\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 35.8039\n",
      "99.45  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "images_path = glob.glob(os.path.join(images_root_dir, '*.jpg'))\n",
    "labels = [int(image_path.split('_')[-1].split('.')[0]) for image_path in images_path]\n",
    "\n",
    "projection_mean, projection_std = extract_normalization_params(transformation)\n",
    "transformed_images = transform_images(images_path, transformation)\n",
    "\n",
    "probs, logits = get_logits_and_probs(model, transformed_images)\n",
    "accepted_margins, failed_margins = calculate_margins(logits, labels)\n",
    "safe_margins = [x[1] for x in\n",
    "                sorted(find_safe_margin(accepted_margins, failed_margins).items(),\n",
    "                       key=lambda x: x[0])]\n",
    "confident_images_per_class = select_top_images_per_class(probs, transformed_images,\n",
    "                                labels, num_classes, top_k=3)\n",
    "k = 1\n",
    "#confident_images_per_class = {c: [generate_random_image(transformation) for i in range(k)]\n",
    "#                              for c in range(num_classes)}\n",
    "\n",
    "max_margins, triggers_per_class, all_margins_per_class = compute_max_margin(model, confident_images_per_class,\n",
    "                                   num_classes, projection_mean, projection_std,\n",
    "                                   max_iterations=500, lr=0.01,\n",
    "                                   tolerance=1e-5,\n",
    "                                   max_img_per_class=2)\n",
    "\n",
    "p_values_standard = compute_p_values(max_margins, distributions=['gamma', 'norm', 'expon'],\n",
    "                                     p_value_type='standard')\n",
    "safe_p_values_standard = compute_p_values(safe_margins, distributions=['gamma', 'norm', 'expon'],\n",
    "                                          p_value_type='standard')\n",
    "\n",
    "print(round(time.time() - t, 2), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b924f2-1b72-4ed6-8c65-c301f641a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 1.6538992397840957e-09, 'norm': 0.0, 'expon': 0.002562337471917253}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f70f8e-c377-4499-8eeb-9d14d50a32ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.3257437515907471,\n",
       " 'norm': 0.06399811442080117,\n",
       " 'expon': 0.1445862582538635}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_p_values_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "015b8397-7e58-4946-bbe8-62a52b5b140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_images_per_class = select_top_images_per_class(probs, transformed_images,\n",
    "                                labels, num_classes, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22577070-beca-4923-a3a2-325e6c96c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted image: clean_num_0\n",
      "Plotted image: clean_num_1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(tensor, mean, std, title=None, save_dir=None, filename=None):\n",
    "    plt.figure()\n",
    "    img = tensor.clone()\n",
    "    if img.dim() == 4:\n",
    "        img = img[0]\n",
    "    num_channels = img.size(0)\n",
    "    if isinstance(mean, (list, tuple)):\n",
    "        if len(mean) == 1:\n",
    "            mean = [mean[0]] * num_channels\n",
    "        elif len(mean) != num_channels:\n",
    "            raise ValueError(f\"Length of mean ({len(mean)}) does not match number of channels ({num_channels}).\")\n",
    "        mean = torch.tensor(mean).to(DEVICE)\n",
    "    else:\n",
    "        mean = torch.tensor([mean] * num_channels).to(DEVICE)\n",
    "    if isinstance(std, (list, tuple)):\n",
    "        if len(std) == 1:\n",
    "            std = [std[0]] * num_channels\n",
    "        elif len(std) != num_channels:\n",
    "            raise ValueError(f\"Length of std ({len(std)}) does not match number of channels ({num_channels}).\")\n",
    "        std = torch.tensor(std).to(DEVICE)\n",
    "    else:\n",
    "        std = torch.tensor([std] * num_channels).to(DEVICE)\n",
    "    img = img.to(DEVICE)\n",
    "    for c in range(num_channels):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    np_img = img.cpu().numpy()\n",
    "    if num_channels == 1:\n",
    "        np_img = np_img.squeeze(0)\n",
    "        plt.imshow(np_img, cmap='gray')\n",
    "    else:\n",
    "        np_img = np.transpose(np_img, (1, 2, 0))\n",
    "        plt.imshow(np_img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if save_dir and filename:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'{filename}.png')\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        LOGGER.info(f\"Saved image plot to {save_path}\")\n",
    "    plt.plot()\n",
    "    #plt.close()\n",
    "    LOGGER.info(f\"Plotted image: {title}\")\n",
    "\n",
    "\n",
    "for i, img in enumerate(confident_images_per_class[5]):\n",
    "    imshow(img, projection_mean, projection_std, f'clean_num_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1d29a13-f2ae-4fe6-a8f4-a8462d8dbf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confident_images_per_class[0][0].shape)\n",
    "triggers_per_class[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d95e30d8-ba56-4abb-95af-ae1f51ae2330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM tensor(94.8370, grad_fn=<SubBackward0>)\n",
      "Plotted image: adversary_num_0\n",
      "MM tensor(89.5917, grad_fn=<SubBackward0>)\n",
      "Plotted image: adversary_num_1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (img, margin) in enumerate(zip(triggers_per_class[5], all_margins_per_class[5])):\n",
    "    print('MM', margin)\n",
    "    imshow(img, projection_mean, projection_std, f'adversary_num_{i}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd9f2c72-1f6f-41ae-af46-8ca7966f9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_ax(ax, tensor, mean, std, title=None):\n",
    "    img = tensor.clone()\n",
    "    if img.dim() == 4:\n",
    "        img = img[0]\n",
    "    num_channels = img.size(0)\n",
    "    \n",
    "    # Handle mean and std\n",
    "    if isinstance(mean, (list, tuple)):\n",
    "        if len(mean) == 1:\n",
    "            mean = [mean[0]] * num_channels\n",
    "        elif len(mean) != num_channels:\n",
    "            raise ValueError(f\"Length of mean ({len(mean)}) does not match number of channels ({num_channels}).\")\n",
    "        mean = torch.tensor(mean).to(DEVICE)\n",
    "    else:\n",
    "        mean = torch.tensor([mean] * num_channels).to(DEVICE)\n",
    "    \n",
    "    if isinstance(std, (list, tuple)):\n",
    "        if len(std) == 1:\n",
    "            std = [std[0]] * num_channels\n",
    "        elif len(std) != num_channels:\n",
    "            raise ValueError(f\"Length of std ({len(std)}) does not match number of channels ({num_channels}).\")\n",
    "        std = torch.tensor(std).to(DEVICE)\n",
    "    else:\n",
    "        std = torch.tensor([std] * num_channels).to(DEVICE)\n",
    "    \n",
    "    img = img.to(DEVICE)\n",
    "    for c in range(num_channels):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    np_img = img.cpu().numpy()\n",
    "    \n",
    "    if num_channels == 1:\n",
    "        np_img = np_img.squeeze(0)\n",
    "        ax.imshow(np_img, cmap='gray')\n",
    "    else:\n",
    "        np_img = np.transpose(np_img, (1, 2, 0))\n",
    "        ax.imshow(np_img)\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "def plot_images_grid(clean_images: List[torch.Tensor],\n",
    "                     adversarial_images: List[torch.Tensor],\n",
    "                     projection_mean: List[float],\n",
    "                     projection_std: List[float],\n",
    "                     save_path: str,\n",
    "                     cols: int = 5):\n",
    "    total_clean = len(clean_images)\n",
    "    total_adversarial = len(adversarial_images)\n",
    "    total = total_clean + total_adversarial\n",
    "    \n",
    "    # Determine grid size\n",
    "    rows = (total + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot clean images\n",
    "    for i, img in enumerate(clean_images):\n",
    "        title = f'Clean #{i}'\n",
    "        imshow_ax(axes[i], img, projection_mean, projection_std, title=title)\n",
    "    \n",
    "    # Plot adversarial images\n",
    "    for i, img in enumerate(adversarial_images):\n",
    "        idx = total_clean + i\n",
    "        title = f'Adversary #{i}'\n",
    "        imshow_ax(axes[idx], img, projection_mean, projection_std, title=title)\n",
    "    \n",
    "    # Turn off any remaining axes if total < rows*cols\n",
    "    for i in range(total, rows * cols):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    LOGGER.info(f\"Saved combined image grid to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3056ed1-b38c-4f69-91f2-6e93c44aaf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined image grid to test.png\n"
     ]
    }
   ],
   "source": [
    "plot_images_grid(confident_images_per_class[5],\n",
    "                 triggers_per_class[5], projection_mean, projection_std, 'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7236aef-62ac-463a-b0d8-75b65787afd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    Grayscale(num_output_channels=3)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.5], std=[0.5])\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b8c9529-1926-44ec-99da-cf9f81210237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb026c96-d9cd-4ae1-a92f-e4121d1b44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def select_neurons_and_get_activations_per_each(model, percent_per_layer, inputs: List[torch.Tensor]):\n",
    "    LOGGER.info(\"Selecting neurons and collecting activations\")\n",
    "    activation = defaultdict(list)\n",
    "    hooks = []\n",
    "    selected_neurons = {}\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            layers.append(name)\n",
    "    for name in layers:\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                act = output.detach()\n",
    "                if act.dim() > 2:\n",
    "                    act = act.mean(dim=[2,3])\n",
    "                act = act.squeeze(0).cpu().numpy()\n",
    "                if act.ndim == 1:\n",
    "                    activation[name].append(act)\n",
    "                else:\n",
    "                    activation[name].append(act.flatten())\n",
    "            return hook\n",
    "        hooks.append(model.get_submodule(name).register_forward_hook(get_activation(name)))\n",
    "    LOGGER.info(\"Registered hooks\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(inputs, desc=\"Processing inputs\"):\n",
    "            model(img.unsqueeze(0))\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    LOGGER.info(\"Collected activations\")\n",
    "    for name in layers:\n",
    "        acts = activation[name]\n",
    "        if len(acts) != len(inputs):\n",
    "            LOGGER.error(f\"Activation length mismatch for layer {name}: expected {len(inputs)}, got {len(acts)}\")\n",
    "            raise ValueError(f\"Activation length mismatch for layer {name}\")\n",
    "        act = np.stack(acts, axis=0)\n",
    "        num_neurons = act.shape[1]\n",
    "        selected = np.random.choice(num_neurons, max(1, int(num_neurons * percent_per_layer)), replace=False)\n",
    "        selected_neurons[name] = selected\n",
    "    LOGGER.info(\"Selected neurons per layer\")\n",
    "    activations_per_input = []\n",
    "    for img_idx in tqdm(range(len(inputs)), desc=\"Aggregating activations\"):\n",
    "        img_activations = []\n",
    "        for name in layers:\n",
    "            act = activation[name][img_idx]\n",
    "            selected = selected_neurons[name]\n",
    "            img_activations.extend(act[selected])\n",
    "        activations_per_input.append(img_activations)\n",
    "    LOGGER.info(\"Aggregated activations per input\")\n",
    "    return activations_per_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "751400f1-246b-4186-a8d6-50fe973e38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ---------------------- Correlation Computation ----------------------\n",
    "\n",
    "def compute_neuron_pairwise_correlation(activations: List[List[float]]) -> np.ndarray:\n",
    "    LOGGER.info(\"Computing neuron pairwise correlation\")\n",
    "    activation_matrix = np.array(activations)  # Shape: (num_inputs, num_neurons)\n",
    "    if activation_matrix.ndim != 2:\n",
    "        LOGGER.error(\"Activation matrix is not 2D\")\n",
    "        raise ValueError(\"Activation matrix must be 2D\")\n",
    "    corr_matrix = np.corrcoef(activation_matrix, rowvar=False)  # Correlation between neurons\n",
    "    LOGGER.info(\"Computed neuron pairwise correlation\")\n",
    "    return corr_matrix\n",
    "\n",
    "# ---------------------- Plotting Functions ----------------------\n",
    "\n",
    "def plot_correlation_heatmap(ax, corr_matrix: np.ndarray, title: str, vmin=-1, vmax=1):\n",
    "    \"\"\"\n",
    "    Plots a single correlation heatmap on the given Axes.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The Axes object to plot the heatmap on.\n",
    "        corr_matrix (np.ndarray): The correlation matrix to visualize.\n",
    "        title (str): Title of the heatmap.\n",
    "        vmin (float, optional): Minimum value for colormap scaling. Defaults to -1.\n",
    "        vmax (float, optional): Maximum value for colormap scaling. Defaults to 1.\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Plotting correlation heatmap: {title}\")\n",
    "    sns.heatmap(corr_matrix, cmap='viridis', annot=False, vmin=vmin, vmax=vmax, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    LOGGER.info(f\"Plotted correlation heatmap: {title}\")\n",
    "\n",
    "def plot_difference_heatmap(ax, corr_diff: np.ndarray, title: str, vmin=-1, vmax=1):\n",
    "    \"\"\"\n",
    "    Plots a difference correlation heatmap on the given Axes.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The Axes object to plot the heatmap on.\n",
    "        corr_diff (np.ndarray): The difference between two correlation matrices.\n",
    "        title (str): Title of the heatmap.\n",
    "        vmin (float, optional): Minimum value for colormap scaling. Defaults to -1.\n",
    "        vmax (float, optional): Maximum value for colormap scaling. Defaults to 1.\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Plotting difference correlation heatmap: {title}\")\n",
    "    sns.heatmap(corr_diff, cmap='bwr', center=0, vmin=vmin, vmax=vmax, annot=False, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    LOGGER.info(f\"Plotted difference correlation heatmap: {title}\")\n",
    "\n",
    "def plot_neuron_pairwise_correlation_heatmaps(axes, activations: List[List[float]], label: str, vmin=-1, vmax=1, save_dir: str = None):\n",
    "    LOGGER.info(f\"Plotting neuron pairwise correlation heatmaps for {label} inputs\")\n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= len(activations):\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        act = activations[idx]\n",
    "        act_array = np.array(act)\n",
    "        if act_array.size < 2:\n",
    "            LOGGER.warning(f\"Not enough neurons to compute pairwise correlation for {label} Input {idx}\")\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        if np.std(act_array) == 0:\n",
    "            corr = np.zeros((len(act_array), len(act_array)))\n",
    "        else:\n",
    "            corr = np.corrcoef(act_array)\n",
    "        if corr.size == 1:\n",
    "            LOGGER.warning(f\"Correlation matrix is scalar for {label} Input {idx}, skipping heatmap.\")\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        sns.heatmap(corr, cmap='coolwarm', annot=False, vmin=vmin, vmax=vmax, ax=ax)\n",
    "        ax.set_title(f'{label} Input {idx}')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if save_dir:\n",
    "            save_path = os.path.join(save_dir, f'{label.lower()}_input_{idx}_pairwise_correlation_heatmap.png')\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            LOGGER.info(f\"Saved neuron pairwise correlation heatmap to {save_path}\")\n",
    "        LOGGER.info(f\"Plotted neuron pairwise correlation heatmap for {label} Input {idx}\")\n",
    "    LOGGER.info(f\"Completed plotting neuron pairwise correlation heatmaps for {label} inputs\")\n",
    "\n",
    "# ---------------------- Correlation Analysis and Plotting ----------------------\n",
    "def calculate_pearson_correlation_and_plot(clean_activations: List[List[float]], anomalous_activations: List[List[float]],\n",
    "                                          clean_corr, anomalous_corr):\n",
    "    \"\"\"\n",
    "    Calculates Pearson correlations and plots correlation heatmaps for clean and anomalous activations.\n",
    "\n",
    "    Args:\n",
    "        clean_activations (List[List[float]]): Activations from clean inputs.\n",
    "        anomalous_activations (List[List[float]]): Activations from anomalous inputs.\n",
    "        clean_corr (np.ndarray): Correlation matrix for clean inputs.\n",
    "        anomalous_corr (np.ndarray): Correlation matrix for anomalous inputs.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\"Calculating Pearson correlations between clean and anomalous activations\")\n",
    "    \n",
    "    # Define save paths\n",
    "    clean_corr_save_path = os.path.join(PLOT_SAVE_DIR, 'clean_correlation_heatmap.png')\n",
    "    anomalous_corr_save_path = os.path.join(PLOT_SAVE_DIR, 'anomalous_correlation_heatmap.png')\n",
    "    difference_corr_save_path = os.path.join(PLOT_SAVE_DIR, 'difference_correlation_heatmap.png')\n",
    "    \n",
    "    # Create a figure with 3 rows for clean, anomalous, and difference heatmaps\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 30))  # Adjust height as needed\n",
    "    plt.tight_layout(pad=5.0)\n",
    "    \n",
    "    # Plot Clean Correlation Heatmap\n",
    "    plot_correlation_heatmap(\n",
    "        axes[0],\n",
    "        clean_corr,\n",
    "        'Neuron Pairwise Correlation Heatmap - Clean Inputs',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    # Plot Anomalous Correlation Heatmap\n",
    "    plot_correlation_heatmap(\n",
    "        axes[1],\n",
    "        anomalous_corr,\n",
    "        'Neuron Pairwise Correlation Heatmap - Anomalous Inputs',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    # Compute and Plot Difference Heatmap\n",
    "    LOGGER.info(\"Computing difference between clean and anomalous correlation matrices\")\n",
    "    corr_diff = anomalous_corr - clean_corr\n",
    "    plot_difference_heatmap(\n",
    "        axes[2],\n",
    "        corr_diff,\n",
    "        'Difference in Neuron Pairwise Correlation (Anomalous - Clean)',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    # Save the entire figure once\n",
    "    plt.savefig(difference_corr_save_path, bbox_inches='tight')\n",
    "    LOGGER.info(f\"Saved all correlation heatmaps to {difference_corr_save_path}\")\n",
    "    \n",
    "    # Close the figure to free memory\n",
    "    plt.close(fig)\n",
    "    LOGGER.info(\"Plotted and saved correlation heatmaps and difference heatmap in separate rows\")\n",
    "    \n",
    "    # Plot neuron pairwise correlation heatmaps for each clean input\n",
    "    num_clean = len(clean_activations)\n",
    "    num_anomalous = len(anomalous_activations)\n",
    "    total_pairwise = num_clean + num_anomalous\n",
    "    if total_pairwise == 0:\n",
    "        LOGGER.warning(\"No neuron pairwise correlation heatmaps to plot\")\n",
    "        return\n",
    "    \n",
    "    # Define subplot grid for pairwise heatmaps\n",
    "    # Clean Inputs\n",
    "    clean_save_dir = os.path.join(PLOT_SAVE_DIR, 'clean_pairwise_heatmaps')\n",
    "    os.makedirs(clean_save_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, act in enumerate(tqdm(clean_activations, desc=\"Plotting clean pairwise heatmaps\")):\n",
    "        fig_clean, ax_clean = plt.subplots(figsize=(10, 8))\n",
    "        if len(act) < 2:\n",
    "            LOGGER.warning(f\"Not enough neurons to compute pairwise correlation for Clean Input {idx}\")\n",
    "            ax_clean.axis('off')\n",
    "        else:\n",
    "            act_array = np.array(act)\n",
    "            if np.std(act_array) == 0:\n",
    "                corr = np.zeros((len(act_array), len(act_array)))\n",
    "            else:\n",
    "                corr = np.corrcoef(act_array)\n",
    "            if corr.size == 1:\n",
    "                LOGGER.warning(f\"Correlation matrix is scalar for Clean Input {idx}, skipping heatmap.\")\n",
    "                ax_clean.axis('off')\n",
    "            else:\n",
    "                sns.heatmap(corr, cmap='coolwarm', annot=False, vmin=-1, vmax=1, ax=ax_clean)\n",
    "                ax_clean.set_title(f'Clean Input #{idx}')\n",
    "                ax_clean.set_xlabel('')\n",
    "                ax_clean.set_ylabel('')\n",
    "                ax_clean.set_xticks([])\n",
    "                ax_clean.set_yticks([])\n",
    "                # Save each heatmap\n",
    "                save_path = os.path.join(clean_save_dir, f'clean_input_{idx}_pairwise_correlation_heatmap.png')\n",
    "                plt.savefig(save_path, bbox_inches='tight')\n",
    "                LOGGER.info(f\"Saved neuron pairwise correlation heatmap to {save_path}\")\n",
    "        plt.close(fig_clean)\n",
    "    \n",
    "    # Anomalous Inputs\n",
    "    anomalous_save_dir = os.path.join(PLOT_SAVE_DIR, 'anomalous_pairwise_heatmaps')\n",
    "    os.makedirs(anomalous_save_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, act in enumerate(tqdm(anomalous_activations, desc=\"Plotting anomalous pairwise heatmaps\")):\n",
    "        fig_anomalous, ax_anomalous = plt.subplots(figsize=(10, 8))\n",
    "        if len(act) < 2:\n",
    "            LOGGER.warning(f\"Not enough neurons to compute pairwise correlation for Anomalous Input {idx}\")\n",
    "            ax_anomalous.axis('off')\n",
    "        else:\n",
    "            act_array = np.array(act)\n",
    "            if np.std(act_array) == 0:\n",
    "                corr = np.zeros((len(act_array), len(act_array)))\n",
    "            else:\n",
    "                corr = np.corrcoef(act_array)\n",
    "            if corr.size == 1:\n",
    "                LOGGER.warning(f\"Correlation matrix is scalar for Anomalous Input {idx}, skipping heatmap.\")\n",
    "                ax_anomalous.axis('off')\n",
    "            else:\n",
    "                sns.heatmap(corr, cmap='coolwarm', annot=False, vmin=-1, vmax=1, ax=ax_anomalous)\n",
    "                ax_anomalous.set_title(f'Anomalous Input #{idx}')\n",
    "                ax_anomalous.set_xlabel('')\n",
    "                ax_anomalous.set_ylabel('')\n",
    "                ax_anomalous.set_xticks([])\n",
    "                ax_anomalous.set_yticks([])\n",
    "                # Save each heatmap\n",
    "                save_path = os.path.join(anomalous_save_dir, f'anomalous_input_{idx}_pairwise_correlation_heatmap.png')\n",
    "                plt.savefig(save_path, bbox_inches='tight')\n",
    "                LOGGER.info(f\"Saved neuron pairwise correlation heatmap to {save_path}\")\n",
    "        plt.close(fig_anomalous)\n",
    "    \n",
    "    LOGGER.info(\"Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf23f9-cea2-439b-b856-0d6b67d38c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce777c3e-6aa8-4ed8-8f73-bac2c1b8bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "\n",
    "PLOT_SAVE_DIR = 'tmp'\n",
    "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs('correlations', exist_ok=True)\n",
    "os.makedirs('corr_files', exist_ok =True)\n",
    "\n",
    "def analyze(idx):\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    model, num_classes, ground_truth, transformation, images_root_dir = load_test(idx)\n",
    "    print('Has Backdoor: ', ground_truth)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    images_path = glob.glob(os.path.join(images_root_dir, '*.jpg'))\n",
    "    labels = [int(image_path.split('_')[-1].split('.')[0]) for image_path in images_path]\n",
    "    \n",
    "    projection_mean, projection_std = extract_normalization_params(transformation)\n",
    "    transformed_images = transform_images(images_path, transformation)\n",
    "    \n",
    "    probs, logits = get_logits_and_probs(model, transformed_images)\n",
    "    accepted_margins, failed_margins = calculate_margins(logits, labels)\n",
    "    safe_margins = [x[1] for x in\n",
    "                    sorted(find_safe_margin(accepted_margins, failed_margins).items(),\n",
    "                           key=lambda x: x[0])]\n",
    "    confident_images_per_class = select_top_images_per_class(probs, transformed_images,\n",
    "                                    labels, num_classes, top_k=3)\n",
    "    k = 1\n",
    "    #confident_images_per_class = {c: [generate_random_image(transformation) for i in range(k)]\n",
    "    #                              for c in range(num_classes)}\n",
    "    \n",
    "    max_margins, triggers_per_class, all_margins_per_class = compute_max_margin(model,\n",
    "                                                                        confident_images_per_class,\n",
    "                                       num_classes, projection_mean, projection_std,\n",
    "                                       max_iterations=500, lr=0.01,\n",
    "                                       tolerance=1e-5,\n",
    "                                       max_img_per_class=2)\n",
    "    \n",
    "    p_values_standard = compute_p_values(max_margins, distributions=['gamma', 'norm', 'expon'],\n",
    "                                         p_value_type='standard')\n",
    "    predicted = p_values_standard['gamma'] <= 0.06\n",
    "    safe_p_values_standard = compute_p_values(safe_margins, distributions=['gamma', 'norm', 'expon'],\n",
    "                                              p_value_type='standard')\n",
    "    \n",
    "    print(round(time.time() - t, 2), ' seconds')\n",
    "    \n",
    "    \n",
    "    c = max_margins.index(max(max_margins))\n",
    "    direc = f'{idx}-{\"backdoor\" if ground_truth else \"clean\"}-predicted-{\"backdoor\" if predicted else \"clean\"}'\n",
    "\n",
    "\n",
    "    #for i, img in enumerate(confident_images_per_class[5]):\n",
    "    #    imshow(img, projection_mean, projection_std, f'clean_num_{i}')\n",
    "    \n",
    "    #for i, (img, margin) in enumerate(zip(triggers_per_class[5], all_margins_per_class[5])):\n",
    "    #    print('MM', margin)\n",
    "    #    imshow(img, projection_mean, projection_std, f'adversary_num_{i}')\n",
    "\n",
    "    plot_images_grid(confident_images_per_class[c], triggers_per_class[c], projection_mean,\n",
    "                    projection_std, f\"correlations/{direc}_samples.png\")\n",
    "\n",
    "    \n",
    "    print('Detected class is', c)\n",
    "    print('All max margins', all_margins_per_class[c])\n",
    "    \n",
    "    porportion = 0.8\n",
    "    clean_images = confident_images_per_class[c]\n",
    "    anomalous_images = triggers_per_class[c]\n",
    "    clean_inputs = [img.to(DEVICE) for img in clean_images]\n",
    "    anomalous_inputs = [img.to(DEVICE) for img in anomalous_images]\n",
    "    print(clean_inputs[0].shape, anomalous_inputs[0].shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if not clean_inputs:\n",
    "        LOGGER.error(\"No clean inputs found\")\n",
    "        raise ValueError(\"No clean inputs found\")\n",
    "    if not anomalous_inputs:\n",
    "        LOGGER.error(\"No anomalous inputs found\")\n",
    "        raise ValueError(\"No anomalous inputs found\")\n",
    "    \n",
    "    # Extract activations\n",
    "    clean_activations = select_neurons_and_get_activations_per_each(model, 0.8, clean_inputs)\n",
    "    anomalous_activations = select_neurons_and_get_activations_per_each(model, 0.8, anomalous_inputs)\n",
    "\n",
    "    clean_correlations = compute_neuron_pairwise_correlation(clean_activations)\n",
    "    anomalous_correlations = compute_neuron_pairwise_correlation(anomalous_activations)\n",
    "\n",
    "    np.save(f'corr_files/{direc}_clean.npy', clean_correlations)\n",
    "    np.save( f'corr_files/{direc}_anom.npy', anomalous_correlations)\n",
    "\n",
    "    # Perform correlation analysis and plotting\n",
    "    calculate_pearson_correlation_and_plot(clean_activations, anomalous_activations,\n",
    "                                           clean_correlations, anomalous_correlations)\n",
    "    shutil.move(f'{PLOT_SAVE_DIR}/difference_correlation_heatmap.png', f'correlations/{direc}_corr.png')\n",
    "\n",
    "    return ground_truth, predicted, clean_correlations, anomalous_activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83987107-124f-4010-bcbc-424023a7ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Backdoor:  True\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 53.3385\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 36.9735\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 109.3607\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 84.9875\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 59.7799\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 51.2974\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 54.5880\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 75.6753\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 133.8008\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 50.3163\n",
      "103.0  seconds\n",
      "Saved combined image grid to correlations/1-backdoor-predicted-backdoor_samples.png\n",
      "Detected class is 8\n",
      "All max margins [tensor(133.8008, grad_fn=<SubBackward0>), tensor(131.6437, grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 121.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 11194.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 152.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 4673.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Computing difference between clean and anomalous correlation matrices\n",
      "Plotting difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Plotted difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Saved all correlation heatmaps to tmp/difference_correlation_heatmap.png\n",
      "Plotted and saved correlation heatmaps and difference heatmap in separate rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting clean pairwise heatmaps:   0%|                                                                                                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix is scalar for Clean Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 1, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 2, skipping heatmap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting clean pairwise heatmaps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 258.15it/s]\n",
      "Plotting anomalous pairwise heatmaps:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix is scalar for Anomalous Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Anomalous Input 1, skipping heatmap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting anomalous pairwise heatmaps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 277.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth, predicted, clean_correlations, anomalous_activations = analyze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28c143ed-6ef7-47e6-a78f-dbbe0294390e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Backdoor:  True\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 48.3635\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 29.3585\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 46.8482\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 55.0265\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 62.5593\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 47.8738\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 58.0101\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 73.0688\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 94.7827\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 44.0806\n",
      "61.96  seconds\n",
      "Saved combined image grid to correlations/0-backdoor-predicted-backdoor_samples.png\n",
      "Detected class is 8\n",
      "All max margins [tensor(94.7827, device='cuda:0', grad_fn=<SubBackward0>), tensor(94.2846, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|██████████████████████████████████████| 3/3 [00:00<00:00, 321.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 3/3 [00:00<00:00, 6459.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|██████████████████████████████████████| 2/2 [00:00<00:00, 327.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 2/2 [00:00<00:00, 6335.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ali/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ali/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed neuron pairwise correlation\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Saved correlation heatmap to tmp/clean_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Saved correlation heatmap to tmp/anomalous_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Computing difference between clean and anomalous correlation matrices\n",
      "Plotting difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Saved difference correlation heatmap to tmp/difference_correlation_heatmap.png\n",
      "Plotted difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Plotted and saved correlation heatmaps and difference heatmap in separate rows\n",
      "Plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Correlation matrix is scalar for Clean Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 1, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 2, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Correlation matrix is scalar for Anomalous Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Anomalous Input 1, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\n",
      "Has Backdoor:  True\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 41.2630\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 36.9735\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 107.6794\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 92.3346\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 54.1661\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 50.4136\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 49.1722\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 69.3999\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 132.3956\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 48.1157\n",
      "63.42  seconds\n",
      "Saved combined image grid to correlations/1-backdoor-predicted-backdoor_samples.png\n",
      "Detected class is 8\n",
      "All max margins [tensor(132.0317, device='cuda:0', grad_fn=<SubBackward0>), tensor(132.3956, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|██████████████████████████████████████| 3/3 [00:00<00:00, 302.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 3/3 [00:00<00:00, 4873.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|██████████████████████████████████████| 2/2 [00:00<00:00, 309.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 2/2 [00:00<00:00, 6021.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ali/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ali/anaconda3/envs/data/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed neuron pairwise correlation\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Saved correlation heatmap to tmp/clean_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Saved correlation heatmap to tmp/anomalous_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Computing difference between clean and anomalous correlation matrices\n",
      "Plotting difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Saved difference correlation heatmap to tmp/difference_correlation_heatmap.png\n",
      "Plotted difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Plotted and saved correlation heatmaps and difference heatmap in separate rows\n",
      "Plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Correlation matrix is scalar for Clean Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 1, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 2, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Correlation matrix is scalar for Anomalous Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Anomalous Input 1, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\n",
      "Has Backdoor:  False\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 33.6760\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 48.2097\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 21.0588\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 39.0394\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 30.2963\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 99.8348\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 21.7455\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 34.4762\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 42.1068\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 37.1622\n",
      "63.01  seconds\n",
      "Saved combined image grid to correlations/2-clean-predicted-backdoor_samples.png\n",
      "Detected class is 5\n",
      "All max margins [tensor(99.8348, device='cuda:0', grad_fn=<SubBackward0>), tensor(92.6295, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|██████████████████████████████████████| 3/3 [00:00<00:00, 290.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 3/3 [00:00<00:00, 4973.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|██████████████████████████████████████| 2/2 [00:00<00:00, 292.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 2/2 [00:00<00:00, 6154.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed neuron pairwise correlation\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Saved correlation heatmap to tmp/clean_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Saved correlation heatmap to tmp/anomalous_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Computing difference between clean and anomalous correlation matrices\n",
      "Plotting difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Saved difference correlation heatmap to tmp/difference_correlation_heatmap.png\n",
      "Plotted difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Plotted and saved correlation heatmaps and difference heatmap in separate rows\n",
      "Plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Correlation matrix is scalar for Clean Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 1, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 2, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Correlation matrix is scalar for Anomalous Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Anomalous Input 1, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\n",
      "Has Backdoor:  False\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 51.9957\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 24.7574\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 61.9017\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 96.7384\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 83.2331\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 40.5889\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 55.2064\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 33.8037\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 151.2767\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 52.1627\n",
      "77.23  seconds\n",
      "Saved combined image grid to correlations/3-clean-predicted-backdoor_samples.png\n",
      "Detected class is 8\n",
      "All max margins [tensor(151.2767, device='cuda:0', grad_fn=<SubBackward0>), tensor(140.5290, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|██████████████████████████████████████| 3/3 [00:00<00:00, 292.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 3/3 [00:00<00:00, 4463.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|██████████████████████████████████████| 2/2 [00:00<00:00, 289.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 2/2 [00:00<00:00, 5581.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed neuron pairwise correlation\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Saved correlation heatmap to tmp/clean_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Saved correlation heatmap to tmp/anomalous_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n",
      "Computing difference between clean and anomalous correlation matrices\n",
      "Plotting difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Saved difference correlation heatmap to tmp/difference_correlation_heatmap.png\n",
      "Plotted difference correlation heatmap: Difference in Neuron Pairwise Correlation (Anomalous - Clean)\n",
      "Plotted and saved correlation heatmaps and difference heatmap in separate rows\n",
      "Plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Correlation matrix is scalar for Clean Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 1, skipping heatmap.\n",
      "Correlation matrix is scalar for Clean Input 2, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Clean inputs\n",
      "Plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Correlation matrix is scalar for Anomalous Input 0, skipping heatmap.\n",
      "Correlation matrix is scalar for Anomalous Input 1, skipping heatmap.\n",
      "Completed plotting neuron pairwise correlation heatmaps for Anomalous inputs\n",
      "Neuron pairwise correlation heatmaps plotted and saved in separate directories without neuron numbers\n",
      "Has Backdoor:  True\n",
      "\n",
      "Processing Class 0/9\n",
      "  Total images to optimize for class 0: 2\n",
      "  Maximum Margin for class 0: 33.1790\n",
      "\n",
      "Processing Class 1/9\n",
      "  Total images to optimize for class 1: 2\n",
      "  Maximum Margin for class 1: 62.1676\n",
      "\n",
      "Processing Class 2/9\n",
      "  Total images to optimize for class 2: 2\n",
      "  Maximum Margin for class 2: 19.8002\n",
      "\n",
      "Processing Class 3/9\n",
      "  Total images to optimize for class 3: 2\n",
      "  Maximum Margin for class 3: 31.4231\n",
      "\n",
      "Processing Class 4/9\n",
      "  Total images to optimize for class 4: 2\n",
      "  Maximum Margin for class 4: 29.0833\n",
      "\n",
      "Processing Class 5/9\n",
      "  Total images to optimize for class 5: 2\n",
      "  Maximum Margin for class 5: 45.9028\n",
      "\n",
      "Processing Class 6/9\n",
      "  Total images to optimize for class 6: 2\n",
      "  Maximum Margin for class 6: 18.5385\n",
      "\n",
      "Processing Class 7/9\n",
      "  Total images to optimize for class 7: 2\n",
      "  Maximum Margin for class 7: 45.9968\n",
      "\n",
      "Processing Class 8/9\n",
      "  Total images to optimize for class 8: 2\n",
      "  Maximum Margin for class 8: 54.7399\n",
      "\n",
      "Processing Class 9/9\n",
      "  Total images to optimize for class 9: 2\n",
      "  Maximum Margin for class 9: 38.2267\n",
      "68.31  seconds\n",
      "Saved combined image grid to correlations/4-backdoor-predicted-clean_samples.png\n",
      "Detected class is 1\n",
      "All max margins [tensor(62.1676, device='cuda:0', grad_fn=<SubBackward0>), tensor(44.2827, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "torch.Size([3, 28, 28]) torch.Size([3, 28, 28])\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing inputs: 100%|██████████████████████████████████████| 3/3 [00:00<00:00, 302.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 3/3 [00:00<00:00, 4830.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Selecting neurons and collecting activations\n",
      "Registered hooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing inputs: 100%|██████████████████████████████████████| 2/2 [00:00<00:00, 303.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected activations\n",
      "Selected neurons per layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating activations: 100%|███████████████████████████████| 2/2 [00:00<00:00, 4715.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated activations per input\n",
      "Calculating Pearson correlations between clean and anomalous activations\n",
      "Computing neuron pairwise correlation\n",
      "Computed neuron pairwise correlation\n",
      "Computing neuron pairwise correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed neuron pairwise correlation\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Saved correlation heatmap to tmp/clean_correlation_heatmap.png\n",
      "Plotted correlation heatmap: Neuron Pairwise Correlation Heatmap - Clean Inputs\n",
      "Plotting correlation heatmap: Neuron Pairwise Correlation Heatmap - Anomalous Inputs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 89\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m anomalous_activations \u001b[38;5;241m=\u001b[39m select_neurons_and_get_activations_per_each(model, \u001b[38;5;241m0.8\u001b[39m, anomalous_inputs)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Perform correlation analysis and plotting\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[43mcalculate_pearson_correlation_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomalous_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPLOT_SAVE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/difference_correlation_heatmap.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_corr.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[66], line 106\u001b[0m, in \u001b[0;36mcalculate_pearson_correlation_and_plot\u001b[0;34m(clean_activations, anomalous_activations)\u001b[0m\n\u001b[1;32m     96\u001b[0m plot_correlation_heatmap(\n\u001b[1;32m     97\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     98\u001b[0m     clean_corr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     save_path\u001b[38;5;241m=\u001b[39mclean_corr_save_path\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Plot Anomalous Correlation Heatmap\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[43mplot_correlation_heatmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43manomalous_corr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNeuron Pairwise Correlation Heatmap - Anomalous Inputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manomalous_corr_save_path\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Compute and Plot Difference Heatmap\u001b[39;00m\n\u001b[1;32m    116\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing difference between clean and anomalous correlation matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[66], line 27\u001b[0m, in \u001b[0;36mplot_correlation_heatmap\u001b[0;34m(ax, corr_matrix, title, vmin, vmax, save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_yticks([])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_inches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved correlation heatmap to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlotted correlation heatmap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/pyplot.py:1134\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/figure.py:3390\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3388\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3389\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[0;32m-> 3390\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/backend_bases.py:2193\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2193\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2041\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2042\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:497\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:445\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    447\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    448\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:388\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    387\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3154\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3158\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/axes/_base.py:3070\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3068\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3070\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3073\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/data/lib/python3.10/site-packages/matplotlib/collections.py:2221\u001b[0m, in \u001b[0;36mQuadMesh.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mdraw_gouraud_triangles(\n\u001b[1;32m   2219\u001b[0m         gc, triangles, colors, transform\u001b[38;5;241m.\u001b[39mfrozen())\n\u001b[1;32m   2220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2221\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_quad_mesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_trf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Backends expect flattened rgba arrays (n*m, 4) for fc and ec\u001b[39;49;00m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_facecolor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_antialiased\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_edgecolors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2228\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m   2229\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7a622746fdf0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/data/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x7a608bf9a710> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7a608bfaaef0> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    analyze(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e18524-fa4d-466c-a900-acd77ebd89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_mean, projection_std = extract_normalization_params(transformation)\n",
    "projection_mean = projection_mean[0]\n",
    "projection_std = projection_std[0]\n",
    "\n",
    "INPUT_SIZE: List = [1, 28, 28] # Input images' shape (default to be MNIST)\n",
    "INPUT_RANGE: List = [-projection_mean/projection_std, (1-projection_mean)/projection_std]   # Input image range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01a0fc5a-47c8-4dae-a626-627d26acb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4156/1340279981.py:18: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.sparse.csr import csr_matrix\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Date    : 2021-12-17 12:00:00\n",
    "# @Author  : Songzhu Zheng (imzszhahahaha@gmail.com)\n",
    "# @Link    : https://songzhu-academic-site.netlify.app/\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "# Total number of neurons to be sampled\n",
    "SAMPLE_LIMIT = 3e3\n",
    "\n",
    "def img_std(img):\n",
    "    \"\"\"\n",
    "    Reshape and rescale the input images to fit the model.\n",
    "    \"\"\"\n",
    "    h, w, c = img.shape\n",
    "    dx = int((w - 224) / 2)\n",
    "    dy = int((w - 224) / 2)\n",
    "    img = img[dy:dy+224, dx:dx+224, :]\n",
    "    # perform tensor formatting and normalization explicitly\n",
    "    # convert to CHW dimension ordering\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    # convert to NCHW dimension ordering\n",
    "    img = np.expand_dims(img, 0)\n",
    "    # normalize the image\n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    # convert image to a gpu tensor\n",
    "    batch_data = torch.FloatTensor(img);\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def parse_arch(model: torch.tensor)-> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Parse a input model to extact layer-wise (Conv2d or Linear) module and corresponding module name.\n",
    "    Input args:\n",
    "        model (torch.nn.Module): A torch network\n",
    "    Return:\n",
    "        layer_list (List): A list contain all Conv2d and Linear module from shallow to deep\n",
    "        layer_k (List): A list contain names of extracted modules in layer_list\n",
    "    \"\"\"\n",
    "\n",
    "    layer_list = []\n",
    "    layer_k = []\n",
    "    for k in model._modules:\n",
    "        if model._modules[k]._modules:\n",
    "            # If it has child module then recursively extract the child module\n",
    "            sub_layer_list, sub_layer_k = parse_arch(model._modules[k])\n",
    "            layer_list += sub_layer_list\n",
    "            layer_k += [k+'_'+x for x in sub_layer_k]\n",
    "        elif isinstance(model._modules[k], torch.nn.Conv2d) or isinstance(model._modules[k], torch.nn.Linear):\n",
    "            layer_list.append(model._modules[k])\n",
    "            layer_k.append(model._modules[k]._get_name())\n",
    "    return layer_list, layer_k\n",
    "\n",
    "\n",
    "def feature_collect(model: torch.tensor, images: torch.tensor)-> Tuple[Dict, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Helper function to collection intermediate output of a model for given inputs.\n",
    "    Input args:\n",
    "        model (torch.nn.Module): A torch network\n",
    "        images (torch.tensor): A valid image torch.tensor\n",
    "    Return:\n",
    "         feature_dict (dict): A dictionary contain all intermediate output tensor whose key is the (layer depth, module name)\n",
    "         output (torch.tensor): final output of model\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    # Hook function to be registered during the forward procedure to collect intermediate output\n",
    "    def feature_hook(module, f_in, f_out):\n",
    "        if isinstance(f_in, torch.Tensor):\n",
    "            outs.append(f_in.detach().cpu())\n",
    "        else:\n",
    "            outs.append(f_in[0].detach().cpu())\n",
    "    module_list, module_k = parse_arch(model)\n",
    "    feature_dict = {}\n",
    "    handle_list = []\n",
    "    # Keep registration handle to remove registration later\n",
    "    for layer_ind in range(len(module_list)):\n",
    "        handle_list.append(module_list[layer_ind].register_forward_hook(hook=feature_hook))\n",
    "    output = model(images)\n",
    "    for layer_ind in range(len(module_list)):\n",
    "        #  if layer_ind in layer_select:\n",
    "        feature_dict[(layer_ind, module_k[layer_ind])] = outs[layer_ind]\n",
    "        handle_list[layer_ind].remove()\n",
    "    return feature_dict, output\n",
    "\n",
    "\n",
    "def sample_act(neural_act: torch.tensor, layer_list: List, sample_size: int)-> Tuple[torch.tensor, List]:\n",
    "    \"\"\"\n",
    "    Stratified sampling certain number of neurons' output given all activating vector of a model.\n",
    "    Input args:\n",
    "        neural_act (torch.tensor): n*d tensor. n is the total number of neurons and d is number of record (input sample size)\n",
    "        layer_list (List): a list contain Conv2d or Linear module of a network. it is the return of parse_arch\n",
    "        sample_size (int): Interger that specifies the number of neurons to be sampled\n",
    "    \"\"\"\n",
    "    conv_nfilters_list=[x.in_channels for x in layer_list[0] if hasattr(x, \"in_channels\")]\n",
    "    linear_nneurons_list=[x.in_features for x in layer_list[0] if hasattr(x, \"in_features\")]\n",
    "    n_neurons_list=conv_nfilters_list+linear_nneurons_list\n",
    "    layer_sample_num = [int(sample_size * x / sum(n_neurons_list)) for x in n_neurons_list]\n",
    "    n_neurons_list=list(np.cumsum(n_neurons_list))\n",
    "    # Stratified sampling for each layer\n",
    "    n_neurons_list=[0]+n_neurons_list\n",
    "    sample_ind=[np.random.choice(range(n_neurons_list[i], n_neurons_list[i+1]), layer_sample_num[i], replace=False)\n",
    "                for i in range(len(n_neurons_list)-1) if layer_sample_num[i]]\n",
    "    sample_n_neurons_list=[len(x) for x in sample_ind]\n",
    "    sample_ind=np.concatenate(sample_ind)\n",
    "\n",
    "    #return neural_act[sample_ind], sample_n_neurons_list\n",
    "    return neural_act, sample_n_neurons_list\n",
    "\n",
    "\n",
    "def process_pd(pd: torch.tensor, layer_list: List, sample_n_neurons_list: List=None)-> torch.tensor:\n",
    "    if not sample_n_neurons_list:\n",
    "        # If the target sampling neurons list is not given then set it to be the whole layer_list\n",
    "        conv_nfilters_list=[x.in_channels for x in layer_list[0] if hasattr(x, \"in_channels\")]\n",
    "        linear_nneurons_list=[x.in_features for x in layer_list[0] if hasattr(x, \"in_features\")]\n",
    "        n_neurons_list=conv_nfilters_list+linear_nneurons_list\n",
    "        n_neurons_list=[0]+list(np.cumsum(n_neurons_list))\n",
    "    else:\n",
    "        n_neurons_list=[0]+list(np.cumsum(sample_n_neurons_list))\n",
    "    maxpool_pd=np.zeros([len(n_neurons_list)-1, len(n_neurons_list)-1])\n",
    "    for i in range(len(n_neurons_list)-1):\n",
    "        for j in range(i, len(n_neurons_list)-1):\n",
    "            if i==j:\n",
    "                maxpool_pd[i,j]=1\n",
    "            else:\n",
    "                block=pd[n_neurons_list[i]:n_neurons_list[i+1], n_neurons_list[j]:n_neurons_list[j+1]]\n",
    "                # maxpool_pd[i,j]=block.max()\n",
    "                block=block.flatten()\n",
    "                per_ind=np.argpartition(block.flatten(), -int(0.4*len(block)))[-int(0.4*len(block)):]\n",
    "                maxpool_pd[i,j]=block[per_ind].mean()\n",
    "                maxpool_pd[j,i]=maxpool_pd[i,j]\n",
    "    return maxpool_pd\n",
    "\n",
    "def makeSparseDM(D: np.array, threshold: float)-> np.array:\n",
    "    \"\"\"\n",
    "    Convert a dense matrix to COO format. All values that are below thresh are set to be 0.\n",
    "    Input args:\n",
    "        D (np.array): matrix to be converted\n",
    "        threshold (float): threshold below which value will be set to 0\n",
    "    Return:\n",
    "        matrix in compressed sparse column format\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    [I, J] = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    I = I[D <= threshold]\n",
    "    J = J[D <= threshold]\n",
    "    V = D[D <= threshold]\n",
    "    return sparse.coo_matrix((V, (I, J)), shape=(N, N)).tocsr()\n",
    "\n",
    "\n",
    "def getGreedyPerm(D: np.array)-> List:\n",
    "    \"\"\"\n",
    "    A Naive O(N^2) algorithm to do furthest points sampling\n",
    "    Input args:\n",
    "        D (np.array):  An NxN distance matrix for points\n",
    "    Return:\n",
    "        lamdas (List): list Insertion radii of all points\n",
    "    \"\"\"\n",
    "\n",
    "    N = D.shape[0]\n",
    "    # By default, takes the first point in the permutation to be the\n",
    "    # first point in the point cloud, but could be random\n",
    "    perm = np.zeros(N, dtype=np.int64)\n",
    "    lambdas = np.zeros(N)\n",
    "    ds = D[0, :]\n",
    "    for i in range(1, N):\n",
    "        idx = np.argmax(ds)\n",
    "        perm[i] = idx\n",
    "        lambdas[i] = ds[idx]\n",
    "        ds = np.minimum(ds, D[idx, :])\n",
    "    return lambdas[perm]\n",
    "\n",
    "\n",
    "def getApproxSparseDM(lambdas: List, eps: float, D: np.array)-> csr_matrix:\n",
    "    \"\"\"\n",
    "    Purpose: To return the sparse edge list with the warped distances, sorted by weight.\n",
    "    Input args:\n",
    "        lambdas (List): insertion radii for points\n",
    "        eps (float): epsilon approximation constant\n",
    "        D (np.array): NxN distance matrix, okay to modify because last time it's used\n",
    "    Return:\n",
    "        DSparse (scipy.sparse): A sparse NxN matrix with the reweighted edges\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    E0 = (1+eps)/eps\n",
    "    E1 = (1+eps)**2/eps\n",
    "\n",
    "    # Create initial sparse list candidates (Lemma 6)\n",
    "    # Search neighborhoods\n",
    "    nBounds = ((eps**2+3*eps+2)/eps)*lambdas\n",
    "\n",
    "    # Set all distances outside of search neighborhood to infinity\n",
    "    D[D > nBounds[:, None]] = np.inf\n",
    "    [I, J] = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    idx = I < J\n",
    "    I = I[(D < np.inf)*(idx == 1)]\n",
    "    J = J[(D < np.inf)*(idx == 1)]\n",
    "    D = D[(D < np.inf)*(idx == 1)]\n",
    "\n",
    "    #Prune sparse list and update warped edge lengths (Algorithm 3 pg. 14)\n",
    "    minlam = np.minimum(lambdas[I], lambdas[J])\n",
    "    maxlam = np.maximum(lambdas[I], lambdas[J])\n",
    "\n",
    "    # Rule out edges between vertices whose balls stop growing before they touch\n",
    "    # or where one of them would have been deleted.  M stores which of these\n",
    "    # happens first\n",
    "    M = np.minimum((E0 + E1)*minlam, E0*(minlam + maxlam))\n",
    "\n",
    "    t = np.arange(len(I))\n",
    "    t = t[D <= M]\n",
    "    (I, J, D) = (I[t], J[t], D[t])\n",
    "    minlam = minlam[t]\n",
    "    maxlam = maxlam[t]\n",
    "\n",
    "    # Now figure out the metric of the edges that are actually added\n",
    "    t = np.ones(len(I))\n",
    "\n",
    "    # If cones haven't turned into cylinders, metric is unchanged\n",
    "    t[D <= 2*minlam*E0] = 0\n",
    "\n",
    "    # Otherwise, if they meet before the M condition above, the metric is warped\n",
    "    D[t == 1] = 2.0*(D[t == 1] - minlam[t == 1]*E0) # Multiply by 2 convention\n",
    "    return sparse.coo_matrix((D, (I, J)), shape=(N, N)).tocsr()\n",
    "\n",
    "\n",
    "def calc_topo_feature(PH: List, dim: int)-> Dict:\n",
    "    \"\"\"\n",
    "    Compute topological feature from the persistent diagram.\n",
    "    Input args:\n",
    "        PH (List) : Persistent diagram\n",
    "        dim (int) : dimension to be focused on\n",
    "    Return:\n",
    "        Dictionary contains topological feature\n",
    "    \"\"\"\n",
    "    pd_dim = PH[dim]\n",
    "    if dim == 0:\n",
    "        pd_dim = pd_dim[:-1]\n",
    "    pd_dim = np.array(pd_dim)\n",
    "    betti = len(pd_dim)\n",
    "    ave_persis = sum(pd_dim[:, 1] - pd_dim[:, 0]) / betti if betti > 0 else 0\n",
    "    ave_midlife = (sum((pd_dim[:, 0] + pd_dim[:, 1]) / 2) / betti) if betti > 0 else 0\n",
    "    med_midlife = np.median((pd_dim[:, 0] + pd_dim[:, 1]) / 2) if betti > 0 else 0\n",
    "    max_persis = (pd_dim[:, 1] - pd_dim[:, 0]).max() if betti > 0 else 0\n",
    "    top_5_persis = np.mean(np.sort(pd_dim[:, 1] - pd_dim[:, 0])[-5:]) if betti > 0 else 0\n",
    "    topo_feature_dict = {\"betti_\" + str(dim): betti,\n",
    "                         \"avepersis_\" + str(dim): ave_persis,\n",
    "                         \"avemidlife_\" + str(dim): ave_midlife,\n",
    "                         \"maxmidlife_\" + str(dim): med_midlife,\n",
    "                         \"maxpersis_\" + str(dim): max_persis,\n",
    "                         \"toppersis_\" + str(dim): top_5_persis}\n",
    "    return topo_feature_dict\n",
    "\n",
    "\n",
    "def mat_discorr_adjacency(X: torch.tensor, Y: torch.tensor = None)-> torch.tensor:\n",
    "    \"\"\"\n",
    "    Distance-correlation matrix calculation in tensor format. Return pairwise distance correlation among all row vectors in X. \n",
    "\n",
    "    Dist-corr between two vector a and b is:\n",
    "\n",
    "        dist-corr(a, b) = (1/d^2)\\sum_{i=1}^d \\sum_{j=1}^d A_{i,j} B_{i, j}\n",
    "\n",
    "        where:\n",
    "            A_{i,j} = a_{i,j} - a_{i, .} - a_{., j} + a_{., .}\n",
    "            B_{i,j} = b_{i,j} - b_{i, .} - b_{., j} + b_{., .}\n",
    "\n",
    "            a_{i,j} = |a_i - a_j|_p\n",
    "            b_{i,j} = |b_i - b_j|_p\n",
    "            a_{i, .} = (1/d) sum_{j=1}^d a_{i, j}\n",
    "            a_{., j} = (1/d) sum_{i=1}^d a_{i, j}\n",
    "            a_{., .} = (1/d^2) sum_{i=1}^d sum_{j=1}^d a_{i, j}\n",
    "\n",
    "    Input args:\n",
    "        X (torch.tensor). n*d. n is the number of neurons and d is the feature dimension.\n",
    "        Y (torch.tensor). Optional.\n",
    "    \"\"\"\n",
    "    n, m = X.shape\n",
    "    # If Y is not given, then calculate distcorr(X, X)\n",
    "    if not Y:\n",
    "        Y = X\n",
    "    # Constrain the size of tensor to be sent to GPU to avoid memory overflow\n",
    "    # Con-comment to use GPU\n",
    "    # if (64*n**2)/(10**9) < 8:\n",
    "    #     X = X.cuda()\n",
    "    #     Y = Y.cuda()\n",
    "    bpd = torch.cdist(X.unsqueeze(2), Y.unsqueeze(2), p=2)\n",
    "    bpd = bpd - bpd.mean(axis=1)[:, None, :] - bpd.mean(axis=2)[:, : , None] + bpd.mean((1, 2))[:, None, None]\n",
    "    pd = torch.mm(bpd.view(n, -1), bpd.view(n, -1).T)\n",
    "    del bpd, X, Y\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pd/=n**2\n",
    "    pd=torch.sqrt(pd)\n",
    "    pd/=(torch.sqrt(torch.diagonal(pd)[None, :]*torch.diagonal(pd)[:, None])+1e-8)\n",
    "    pd.fill_diagonal_(1)\n",
    "\n",
    "    return pd\n",
    "\n",
    "# TODO: finish all following doc\n",
    "def mat_bc_adjacency(X):\n",
    "    '''\n",
    "    Bhattacharyya correlation matrix version. Return pairwise BC correlation among all row vectors in X. \n",
    "\n",
    "    BC-corr between two vector a and b is:\n",
    "        BC(a, b) = \\sum_i^d \\sqrt{a_i*b_i}\n",
    "\n",
    "    Input args:\n",
    "        X (torch.tensor). n*d. n is the number of neurons and d is the feature dimension.\n",
    "        Y (torch.tensor). Optional.\n",
    "    '''\n",
    "    \n",
    "    if torch.any(X < 0):\n",
    "        raise ValueError('Each value shoule in the range [0,1]')\n",
    "\n",
    "    X = X.cuda()\n",
    "    X_sqrt = torch.sqrt(X)\n",
    "    return torch.matmul(X_sqrt, X_sqrt.T)\n",
    "\n",
    "def mat_cos_adjacency(X):\n",
    "    '''\n",
    "    Cosine similarity matrix version. Return pairwise cos correlation among all row vectors in X. \n",
    "\n",
    "    Input args:\n",
    "        X (torch.tensor). n*d. n is the number of neurons and d is the feature dimension.\n",
    "        Y (torch.tensor). Optional.\n",
    "    '''\n",
    "    X = X.cuda()\n",
    "    X_row_l2_norm = torch.norm(X, p=2, dim=1).view(-1, 1)\n",
    "    X_row_std = X/(X_row_l2_norm+1e-4)\n",
    "    return torch.matmul(X_row_std, X_row_std.T)\n",
    "\n",
    "def mat_pearson_adjacency(X):\n",
    "    '''\n",
    "    Cosine similarity matrix version. Return pairwise Pearson correlation among all row vectors in X. \n",
    "\n",
    "    Input args:\n",
    "        X (torch.tensor). n*d. n is the number of neurons and d is the feature dimension.\n",
    "        Y (torch.tensor). Optional.\n",
    "    '''\n",
    "    X = X.cuda()\n",
    "    X = X - X.mean(1).view(-1, 1)\n",
    "    cov = torch.matmul(X, X.T)\n",
    "    eps = torch.tensor(1e-4).cuda()\n",
    "    sigma = torch.maximum(torch.sqrt(torch.diagonal(cov)), eps)+1e-4\n",
    "    corr  = cov/sigma.view(-1, 1)/sigma.view(1, -1)\n",
    "    corr.fill_diagonal_(1)\n",
    "    return corr\n",
    "\n",
    "def mat_jsdiv_adjacency(X):\n",
    "    '''\n",
    "    Jensen-Shannon Divergence matrix version. Return pairwise JS divergence among all row vectors in X. \n",
    "\n",
    "    The JS divergence between two vector a and b is:\n",
    "        JS(a, b) = 1/2*(KL(a||m)+KL(b||m))\n",
    "\n",
    "        where:\n",
    "            m = (a+b)/2\n",
    "            KL is the Kullback-Leibler divergence\n",
    "\n",
    "    Input args:\n",
    "        X (torch.tensor). n*d. n is the number of neurons and d is the feature dimension.\n",
    "        Y (torch.tensor). Optional.\n",
    "    '''\n",
    "\n",
    "    if torch.any(X < 0):\n",
    "        raise ValueError('Each value shoule in the range [0,1]')\n",
    "\n",
    "    paq = X[:, :, None] + X.T[None, :, :]\n",
    "    logpaq  = torch.log(paq+1e-4)\n",
    "    paqdiag = torch.diag((paq/2*torch.log(paq/2+1e-4)).sum(1)).flatten()\n",
    "    return 1/2*(paqdiag[:, None]+paqdiag[None, :]-(paq*torch.log(paq/2+1e-4)).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee09d076-b0e2-4f14-8ecd-d091c613ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import copy\n",
    "\n",
    "from ripser import Rips\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "def topo_psf_feature_extract(model: torch.nn.Module, example_dict: Dict, psf_config: Dict)-> Dict:\n",
    "    \"\"\"\n",
    "    Extract topological features from a given torch model.\n",
    "    Input args:\n",
    "        model (torch.nn.Module). Target model.\n",
    "        example_dict (Dict). Optional. Dictionary contains clean input examples. If None then all blank images are used.\n",
    "    Return:\n",
    "        fv (Dict). Dictionary contains extracted features\n",
    "    \"\"\"\n",
    "    step_size=psf_config['step_size']\n",
    "    stim_level=psf_config['stim_level']\n",
    "    patch_size=psf_config['patch_size']\n",
    "    input_shape=psf_config['input_shape']\n",
    "    input_valuerange=psf_config['input_range']\n",
    "    n_neuron_sample=psf_config['n_neuron']\n",
    "    method=psf_config['corr_method']\n",
    "    device=psf_config['device']\n",
    "\n",
    "    # If true input examples are not given, use all blank images instead\n",
    "    if not example_dict:\n",
    "        example_dict=defaultdict(list)\n",
    "        example_dict[0].append(torch.zeros(input_shape).unsqueeze(0))\n",
    "\n",
    "    #model=model.to(device)\n",
    "    test_input=example_dict[0][0].to(device)\n",
    "    num_classes=int(model(test_input.unsqueeze(0)).shape[1])\n",
    "\n",
    "    stim_seq=np.linspace(input_valuerange[0], input_valuerange[1], stim_level)\n",
    "    # 2 represent score and conf\n",
    "    feature_map_h=len(range(0, input_shape[1]-patch_size+1, step_size))\n",
    "    feature_map_w=len(range(0, input_shape[2]-patch_size+1, step_size))\n",
    "    print(feature_map_h, feature_map_w)\n",
    "    # PSF feature dim : 2*m*h*w*L*C\n",
    "    #  2: logits and confidence\n",
    "    #  m: numebr of input examples\n",
    "    #  h: feature map height\n",
    "    #  w: feature map width\n",
    "    #  L: number of stimulation levels\n",
    "    #  C: number of classes\n",
    "    psf_feature_pos=torch.zeros(\n",
    "        2,\n",
    "        len(example_dict.keys()),\n",
    "        feature_map_h, feature_map_w,\n",
    "        len(stim_seq), num_classes)\n",
    "    # 12 is the number of topological features (including dim1 and dim2 features)\n",
    "    topo_feature_pos=torch.zeros(\n",
    "        len(example_dict.keys()),\n",
    "        len(range(0, int(feature_map_h*feature_map_w))),\n",
    "        12\n",
    "    )\n",
    "\n",
    "    PH_list=[]\n",
    "    PD_list=[]\n",
    "    rips = Rips(verbose=False)\n",
    "    model=model.to('cuda')\n",
    "    progress=0\n",
    "\n",
    "    out_corr_per_class = dict()\n",
    "    # For each class input examples, scan through pixels with step_size and modify corresponding pixel with different stimulation level.\n",
    "    # Forward all these modified images to the network and collect output logits and confidence\n",
    "    for c in example_dict:\n",
    "        images_count_in_class = len(example_dict[c])\n",
    "        out_corr = None\n",
    "        for img_id in range(images_count_in_class):\n",
    "            input_eg=copy.deepcopy(example_dict[c][img_id])\n",
    "            feature_w_pos=0\n",
    "\n",
    "            \n",
    "            \n",
    "            for pos_w in range(0, input_shape[1]-patch_size+1, step_size):\n",
    "                feature_h_pos = 0\n",
    "                for pos_h in tqdm(range(0, input_shape[2]-patch_size+1, step_size)):\n",
    "                    t0=time.time()\n",
    "                    count=0\n",
    "                    prob_input=input_eg.repeat(len(stim_seq),1,1,1)\n",
    "                    for i in stim_seq:\n",
    "                        prob_input[count,:,\n",
    "                                   int(pos_w):min(int(pos_w+patch_size), input_shape[1]),\n",
    "                                   int(pos_h):min(int(pos_h+patch_size), input_shape[1])]=i\n",
    "                        count+=1\n",
    "                    pred=[]\n",
    "                    batch_size=8 if len(prob_input)>=32 else 1\n",
    "                    if batch_size==1:\n",
    "                        prob_input=prob_input.to(device)\n",
    "                        feature_dict_c, output = feature_collect(model, prob_input)\n",
    "                        pred.append(output.detach().cpu())\n",
    "                    else:\n",
    "                        print('feature_collect')\n",
    "                        for b in tqdm(range(int(len(prob_input)/batch_size))):\n",
    "                            prob_input_batch=prob_input[(8*b):min(8*(b+1), len(prob_input))].to(device)\n",
    "                            feature_dict_c, output = feature_collect(model, prob_input_batch)\n",
    "                            pred.append(output.detach().cpu())\n",
    "                    pred=torch.cat(pred)\n",
    "                    psf_score=pred\n",
    "                    psf_conf=torch.nn.functional.softmax(psf_score, 1)\n",
    "    \n",
    "                    psf_feature_pos[0, c, feature_w_pos, feature_h_pos]=psf_score\n",
    "                    psf_feature_pos[1, c, feature_w_pos, feature_h_pos]=psf_conf\n",
    "    \n",
    "                    # Extract intermediate activating vectors\n",
    "                    neural_act = []\n",
    "                    for k in feature_dict_c:\n",
    "                        if len(feature_dict_c[k][0].shape)==3:\n",
    "                            layer_act = [feature_dict_c[k][i].max(1)[0].max(1)[0].unsqueeze(1) for i in range(len(feature_dict_c[k]))]\n",
    "                        else:\n",
    "                            layer_act = [feature_dict_c[k][i].unsqueeze(1) for i in range(len(feature_dict_c[k]))]\n",
    "                        layer_act=torch.cat(layer_act, dim=1)\n",
    "                        # Standardize the activation layer-wisely\n",
    "                        layer_act=(layer_act-layer_act.mean(1, keepdim=True))/(layer_act.std(1, keepdim=True)+1e-30)\n",
    "                        neural_act.append(layer_act)\n",
    "                    neural_act=torch.cat(neural_act)\n",
    "                    layer_list=parse_arch(model)\n",
    "                    sample_n_neurons_list=None\n",
    "                    if len(neural_act)>1.5e3:\n",
    "                        neural_act, sample_n_neurons_list=sample_act(neural_act, layer_list, sample_size=n_neuron_sample)\n",
    "                    \n",
    "                    if method=='distcorr':\n",
    "                        neural_pd=mat_discorr_adjacency(neural_act)\n",
    "                    elif method=='bc':\n",
    "                        neural_act=torch.softmax(neural_act, 1)\n",
    "                        neural_pd=mat_bc_adjacency(neural_act)\n",
    "                    elif method=='cos':\n",
    "                        neural_pd=mat_cos_adjacency(neural_act)\n",
    "                    elif method=='pearson':\n",
    "                        neural_pd=mat_pearson_adjacency(neural_act)\n",
    "                    elif method=='js':\n",
    "                        neural_act=torch.softmax(neural_act, 1)\n",
    "                        neural_pd=mat_jsdiv_adjacency(neural_act)\n",
    "                    else:\n",
    "                        raise Exception(f\"Correlation metrics {method} doesn't implemented !\")\n",
    "                    if out_corr is None:\n",
    "                        out_corr = neural_pd.detach().cpu().numpy() / images_count_in_class\n",
    "                    else:\n",
    "                        out_corr += neural_pd.detach().cpu().numpy() / images_count_in_class\n",
    "                    D=1-neural_pd.detach().cpu().numpy() if method!='bc' else -np.log(neural_pd.detach().cpu().numpy()+1e-6)\n",
    "                    PD_list.append(neural_pd.detach().cpu().numpy())\n",
    "    \n",
    "                    # Approaximate sparse filtration to further save some computation\n",
    "                    if model._get_name=='ModdedLeNet5Net':\n",
    "                        PH=rips.fit_transform(D, distance_matrix=True)\n",
    "                    else:\n",
    "                        lambdas=getGreedyPerm(D)\n",
    "                        D = getApproxSparseDM(lambdas, 0.1, D)\n",
    "                        PH=rips.fit_transform(D, distance_matrix=True)\n",
    "    \n",
    "                    PH[0]=np.array(PH[0])\n",
    "                    PH[1]=np.array(PH[1])\n",
    "                    PH[0][np.where(PH[0]==np.inf)]=1\n",
    "                    PH[1][np.where(PH[1]==np.inf)]=1\n",
    "                    PH_list.append(PH)\n",
    "                    # Compute the topological feature with the persistent diagram\n",
    "                    clean_feature_0=calc_topo_feature(PH, 0)\n",
    "                    clean_feature_1=calc_topo_feature(PH, 1)\n",
    "                    topo_feature=[]\n",
    "                    for k in sorted(list(clean_feature_0)):\n",
    "                        topo_feature.append(clean_feature_0[k])\n",
    "                    for k in sorted(list(clean_feature_1)):\n",
    "                        topo_feature.append(clean_feature_1[k])\n",
    "                    topo_feature=torch.tensor(topo_feature)\n",
    "                    topo_feature_pos[c, int(feature_w_pos*feature_map_w+feature_h_pos), :]=topo_feature\n",
    "                    feature_h_pos+=1\n",
    "    \n",
    "                feature_w_pos+=1\n",
    "        out_corr_per_class[c] = out_corr\n",
    "\n",
    "    fv={}\n",
    "    fv['psf_feature_pos']=psf_feature_pos\n",
    "    fv['topo_feature_pos']=topo_feature_pos\n",
    "    fv['correlation_matrix']=np.vstack([x[None, :, :] for x in PD_list]).mean(0)\n",
    "    return fv, out_corr_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "698cb1a3-182a-4f1d-802b-f3ae05d091b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    Grayscale(num_output_channels=3)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.5], std=[0.5])\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205f368-d4ae-40e3-8722-7f261f3b1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE:  int = 20 # Stimulation stepsize used in PSF\n",
    "PATCH_SIZE: int = 20 # Stimulation patch size used in PSF\n",
    "STIM_LEVEL: int = 2 # Number of stimulation level used in PSF\n",
    "N_SAMPLE_NEURONS: int = 1.5e3  # Number of neurons for sampling\n",
    "USE_EXAMPLE: bool =  False     # Whether clean inputs will be given or not\n",
    "CORR_METRIC: str = 'distcorr'\n",
    "\n",
    "psf_config = {}\n",
    "psf_config['step_size'] = STEP_SIZE\n",
    "psf_config['stim_level'] = STIM_LEVEL\n",
    "psf_config['patch_size'] = PATCH_SIZE\n",
    "psf_config['input_shape'] = INPUT_SIZE\n",
    "psf_config['input_range'] = INPUT_RANGE\n",
    "psf_config['n_neuron'] = N_SAMPLE_NEURONS\n",
    "psf_config['corr_method'] = CORR_METRIC\n",
    "psf_config['device'] = DEVICE\n",
    "\n",
    "psf_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63234cab-0542-4e19-a371-2c103a32f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/1 [00:31<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fv, out_corr_per_class = topo_psf_feature_extract(model, confident_images_per_class, psf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48de1ae5-9fce-4f17-a5b0-a2c111df766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -0.9216, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -0.9137,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.9529, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "        [[-1.0000, -0.9216, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -0.9137,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.9529, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "        [[-1.0000, -0.9216, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -0.9137,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.9529, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confident_images_per_class[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f088e7-e099-4b00-b174-b53a8ec128e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d51150-96b2-4740-a85e-3b9bfafc9fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
