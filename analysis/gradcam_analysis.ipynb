{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b179fa5d-4da1-4176-9ad0-a8ac798791c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b179fa5d-4da1-4176-9ad0-a8ac798791c4",
        "outputId": "c0bae859-a0e6-4613-ac3a-ea316a0ea494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device is cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:device is cuda\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "LOGGER = logging.getLogger('detector')\n",
        "LOGGER.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "stream_handler = logging.StreamHandler(sys.stdout)\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "LOGGER.addHandler(stream_handler)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LOGGER.info(f'device is {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d61d82-891f-4c4a-a661-98b1df3a8a0f",
      "metadata": {
        "id": "80d61d82-891f-4c4a-a661-98b1df3a8a0f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.ind = None\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        if self.ind is not None:\n",
        "            out += shortcut[:, self.ind, :, :]\n",
        "        else:\n",
        "            out += shortcut\n",
        "        return out\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            BLOCK = block(self.in_planes, planes, stride)\n",
        "            BLOCK.to('cuda')\n",
        "            layers.append(BLOCK)\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(num_classes=10):\n",
        "    return PreActResNet(PreActBlock, [2, 2, 2, 2], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacc0b89-4e43-4454-af3e-7ea45b96503e",
      "metadata": {
        "id": "cacc0b89-4e43-4454-af3e-7ea45b96503e"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "ROOT_DIR = 'eval_dataset'\n",
        "\n",
        "def load_model(num_classes, model_path):\n",
        "    model = PreActResNet18(num_classes)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_test(idx: int):\n",
        "    test_root_dir = os.path.join(ROOT_DIR, str(idx))\n",
        "\n",
        "    metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
        "\n",
        "    num_classes = metadata['num_classes']\n",
        "    ground_truth = metadata['ground_truth']\n",
        "    images_root_dir = metadata['test_images_folder_address']\n",
        "    transformation = metadata['transformation']\n",
        "\n",
        "    model_path = os.path.join(test_root_dir, 'model.pt')\n",
        "\n",
        "    if images_root_dir[0] == '.':\n",
        "        images_root_dir = images_root_dir[2:]\n",
        "\n",
        "    images_root_dir = os.path.join(test_root_dir, images_root_dir)\n",
        "\n",
        "\n",
        "    model = load_model(num_classes, model_path)\n",
        "\n",
        "    return model, num_classes, ground_truth, transformation, images_root_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888ee15d-380e-4d6e-a627-979cb9fa246b",
      "metadata": {
        "id": "888ee15d-380e-4d6e-a627-979cb9fa246b"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def transform_images(images_path: List[str], transformation: transforms.Compose):\n",
        "    transformed_images = []\n",
        "    for img_path in images_path:\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = transformation(image)\n",
        "            transformed_images.append(image)\n",
        "        except Exception as e:\n",
        "            LOGGER.error(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    if not transformed_images:\n",
        "        LOGGER.error(\"No images were loaded. Please check the images_path list.\")\n",
        "\n",
        "    return torch.stack(transformed_images).to(DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ceaf43a-0da4-4060-bf73-f6a795810567",
      "metadata": {
        "id": "2ceaf43a-0da4-4060-bf73-f6a795810567"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_normalization_params(transformation: transforms.Compose):\n",
        "    mean = None\n",
        "    std = None\n",
        "\n",
        "    for transform in transformation.transforms:\n",
        "        if isinstance(transform, transforms.Normalize):\n",
        "            mean = transform.mean\n",
        "            std = transform.std\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def get_logits_and_probs(model: PreActResNet, transformed_images: torch.Tensor):\n",
        "    logits = model(transformed_images)\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    return probabilities, logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28796165-0ffb-4fd8-8a80-3c0ddaee6fab",
      "metadata": {
        "id": "28796165-0ffb-4fd8-8a80-3c0ddaee6fab"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def calculate_margins(probs: List[torch.Tensor],\n",
        "                      labels: List[int]) -> Tuple[defaultdict[int, List[float]],\n",
        "                                                           defaultdict[int, List[float]]]:\n",
        "    accepted_margins = defaultdict(list)\n",
        "    failed_margins = defaultdict(list)\n",
        "    for i in range(len(probs)):\n",
        "        topk = torch.topk(probs[i], k=2, largest=True, sorted=True)\n",
        "        topk_values = topk.values\n",
        "\n",
        "        margin = topk_values[0].item() - topk_values[1].item()\n",
        "        if labels[i] == torch.argmax(probs[i]).item():\n",
        "            accepted_margins[labels[i]].append(margin)\n",
        "        else:\n",
        "            failed_margins[torch.argmax(probs[i]).item()].append(margin)\n",
        "    return accepted_margins, failed_margins\n",
        "\n",
        "def find_safe_margin(accepted_margins: defaultdict[int, List[float]],\n",
        "                     failed_margins: defaultdict[int, List[float]]):\n",
        "    min_accepted_margins = dict()\n",
        "    for c in accepted_margins.keys():\n",
        "        max_failed = float('inf')\n",
        "        if failed_margins[c]:\n",
        "            max_failed = max(failed_margins[c])\n",
        "\n",
        "        min_accepted_margin = max_failed\n",
        "        for margin in accepted_margins[c]:\n",
        "            if margin <= min_accepted_margin:\n",
        "                min_accepted_margin = margin\n",
        "\n",
        "        min_accepted_margins[c] = min_accepted_margin\n",
        "    return min_accepted_margins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aeb0be9-d52e-4470-9485-28d51ea73034",
      "metadata": {
        "id": "0aeb0be9-d52e-4470-9485-28d51ea73034"
      },
      "outputs": [],
      "source": [
        "def project_image(optimized_img: torch.Tensor, mean: float, std: float):\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1).to(optimized_img.device)\n",
        "    std = torch.tensor(std).view(-1, 1, 1).to(optimized_img.device)\n",
        "\n",
        "    min_val = (0.0 - mean) / std\n",
        "    max_val = (1.0 - mean) / std\n",
        "\n",
        "    optimized_img = torch.clamp(optimized_img, min=min_val, max=max_val)\n",
        "\n",
        "    return optimized_img\n",
        "\n",
        "import torch\n",
        "\n",
        "def project_image_with_epsilon(\n",
        "    optimized_img: torch.Tensor,\n",
        "    reference_img: torch.Tensor,\n",
        "    mean: float,\n",
        "    std: float,\n",
        "    epsilon: float\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    # Normalize the reference image\n",
        "    mean_tensor = torch.tensor(mean).view(-1, 1, 1).to(optimized_img.device)\n",
        "    std_tensor = torch.tensor(std).view(-1, 1, 1).to(optimized_img.device)\n",
        "    normalized_ref = (reference_img - mean_tensor) / std_tensor\n",
        "\n",
        "    # Clamp the optimized image to valid pixel range\n",
        "    min_val = (0.0 - mean_tensor) / std_tensor\n",
        "    max_val = (1.0 - mean_tensor) / std_tensor\n",
        "    optimized_img = torch.clamp(optimized_img, min=min_val, max=max_val)\n",
        "\n",
        "    # Clamp the optimized image to be within epsilon distance from the reference\n",
        "    optimized_img = torch.max(optimized_img, normalized_ref - epsilon)\n",
        "    optimized_img = torch.min(optimized_img, normalized_ref + epsilon)\n",
        "\n",
        "    return optimized_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e00280-2201-4ff8-b8fe-0e95267837cb",
      "metadata": {
        "id": "22e00280-2201-4ff8-b8fe-0e95267837cb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def select_top_images_per_class(probs: List[torch.Tensor],\n",
        "                                images: List[torch.Tensor],\n",
        "                                labels: List[int],\n",
        "                                num_classes: int, top_k=3) -> defaultdict[int, List[torch.Tensor]]:\n",
        "\n",
        "    selected_images_per_class = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (prob, label) in enumerate(zip(probs, labels)):\n",
        "            detected_class = torch.argmax(prob).item()\n",
        "            if detected_class == label:\n",
        "                confidence = prob[detected_class].item()\n",
        "                selected_images_per_class[label].append((confidence, images[idx]))\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        class_images = selected_images_per_class[c]\n",
        "        sorted_images = sorted(class_images, key=lambda x: x[0], reverse=True)\n",
        "        selected_images_per_class[c] = [img for _, img in sorted_images[:top_k]]\n",
        "    return selected_images_per_class\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcde240-35e5-451e-87b2-24148dd0fd59",
      "metadata": {
        "id": "0dcde240-35e5-451e-87b2-24148dd0fd59"
      },
      "outputs": [],
      "source": [
        "def generate_random_image(transformation):\n",
        "    transformation = transforms.Compose([transforms.ToPILImage()] + list(transformation.transforms))\n",
        "    return transformation(torch.randn(3, *(220, 220)) * 255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c330206-e589-4ef5-8a6b-7ad4e988c568",
      "metadata": {
        "id": "4c330206-e589-4ef5-8a6b-7ad4e988c568"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def compute_max_margin(model: PreActResNet,\n",
        "                       selected_images_per_class: dict[int, List],\n",
        "                       num_classes: int,\n",
        "                       projection_mean: float, projection_std: float,\n",
        "                       max_iterations=1000, lr=0.01, tolerance=1e-5, max_img_per_class=3):\n",
        "    model.eval()\n",
        "    max_margins = {}\n",
        "    all_margins_per_class = defaultdict(list)\n",
        "    all_triggers_per_class = defaultdict(list)\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        margins = []\n",
        "        triggers = []\n",
        "\n",
        "        LOGGER.info(f\"\\nProcessing Class {c}/{num_classes - 1}\")\n",
        "        images_to_optimize = []\n",
        "        for k in range(num_classes):\n",
        "            if k == c:\n",
        "                continue\n",
        "            images = selected_images_per_class.get(k, [])\n",
        "            if not images:\n",
        "                continue\n",
        "            images_to_optimize.extend(images)\n",
        "        random.shuffle(images_to_optimize)\n",
        "        images_to_optimize = images_to_optimize[:max_img_per_class]\n",
        "\n",
        "        LOGGER.info(f\"  Total images to optimize for class {c}: {len(images_to_optimize)}\")\n",
        "\n",
        "        for idx, img in enumerate(images_to_optimize):\n",
        "            max_margin = -float('inf')\n",
        "            trigger = None\n",
        "\n",
        "            optimized_img = img.clone().detach().to(DEVICE)\n",
        "            optimized_img.requires_grad = True\n",
        "\n",
        "            if lr != None:\n",
        "                optimizer = torch.optim.Adam([optimized_img], lr=lr)\n",
        "            else:\n",
        "                optimizer = torch.optim.Adam([optimized_img])\n",
        "\n",
        "            f_old = None\n",
        "\n",
        "            for iteration in range(max_iterations):\n",
        "                optimizer.zero_grad()\n",
        "                probs, logits = get_logits_and_probs(model, optimized_img.unsqueeze(0))\n",
        "                logits = logits.squeeze(0)\n",
        "                g_c = logits[c]\n",
        "\n",
        "                mask = torch.ones(num_classes, dtype=torch.bool).to(DEVICE)\n",
        "                mask[c] = False\n",
        "                g_k = torch.max(logits[mask])\n",
        "\n",
        "                margin = g_c - g_k\n",
        "                if margin > max_margin:\n",
        "                    max_margin = margin\n",
        "                    trigger = optimized_img.detach().clone()\n",
        "\n",
        "                loss = -margin\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    optimized_img.copy_(project_image(optimized_img, projection_mean, projection_std))\n",
        "\n",
        "                f_new = margin.item()\n",
        "                if f_old is not None:\n",
        "                    relative_change = abs(f_new - f_old) / (abs(f_old) + 1e-8)\n",
        "                    if relative_change < tolerance:\n",
        "                        break\n",
        "                f_old = f_new\n",
        "            margins.append(max_margin)\n",
        "            triggers.append(trigger)\n",
        "\n",
        "\n",
        "        if margins:\n",
        "            all_triggers_per_class[c] = triggers.copy()\n",
        "            all_margins_per_class[c] = margins.copy()\n",
        "            max_margins[c] = max(margins)\n",
        "            LOGGER.info(f\"  Maximum Margin for class {c}: {max_margins[c]:.4f}\")\n",
        "        else:\n",
        "            max_margins[c] = 0.0\n",
        "            LOGGER.info(f\"  No margins computed for class {c}.\")\n",
        "\n",
        "    max_margins = [x[1].item() for x in sorted(max_margins.items(), key=lambda x: x[0])]\n",
        "\n",
        "    return max_margins, all_triggers_per_class, all_margins_per_class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ffd814-13aa-403a-87f9-a51e19eec320",
      "metadata": {
        "id": "68ffd814-13aa-403a-87f9-a51e19eec320"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "\n",
        "def compute_max_margin(\n",
        "    model: PreActResNet,\n",
        "    selected_images_per_class: Dict[int, List[torch.Tensor]],\n",
        "    num_classes: int,\n",
        "    projection_mean: float,\n",
        "    projection_std: float,\n",
        "    max_iterations: int = 1000,\n",
        "    lr: float = 0.01,\n",
        "    tolerance: float = 1e-5,\n",
        "    max_img_per_class: int = 3,\n",
        "    device: str = DEVICE  # Ensure DEVICE is defined\n",
        ") -> Tuple[List[float], Dict[int, List[torch.Tensor]], Dict[int, List[float]]]:\n",
        "    model.eval()\n",
        "    max_margins = {}\n",
        "    all_margins_per_class = defaultdict(list)\n",
        "    all_triggers_per_class = defaultdict(list)\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        LOGGER.info(f\"\\nProcessing Class {c}/{num_classes - 1}\")\n",
        "        images_to_optimize = []\n",
        "        for k in range(num_classes):\n",
        "            if k == c:\n",
        "                continue\n",
        "            images = selected_images_per_class.get(k, [])\n",
        "            if not images:\n",
        "                continue\n",
        "            images_to_optimize.extend(images)\n",
        "        random.shuffle(images_to_optimize)\n",
        "        images_to_optimize = images_to_optimize[:max_img_per_class]\n",
        "\n",
        "        if not images_to_optimize:\n",
        "            max_margins[c] = 0.0\n",
        "            LOGGER.info(f\"  No images to optimize for class {c}.\")\n",
        "            continue\n",
        "\n",
        "        LOGGER.info(f\"  Total images to optimize for class {c}: {len(images_to_optimize)}\")\n",
        "\n",
        "        # Create a batch of images\n",
        "        batch_size = len(images_to_optimize)\n",
        "        optimized_imgs = torch.stack([img.clone().detach() for img in images_to_optimize]).to(device)\n",
        "        optimized_imgs.requires_grad = True\n",
        "\n",
        "        optimizer = Adam([optimized_imgs], lr=lr) if lr is not None else Adam([optimized_imgs])\n",
        "\n",
        "        max_margins_batch = torch.full((batch_size,), -float('inf'), device=device)\n",
        "        triggers_batch = optimized_imgs.clone().detach()\n",
        "\n",
        "        f_old = torch.full((batch_size,), float('inf'), device=device)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            probs, logits = get_logits_and_probs(model, optimized_imgs)  # Assume batch processing\n",
        "            # logits shape: (batch_size, num_classes)\n",
        "            g_c = logits[:, c]  # Shape: (batch_size,)\n",
        "\n",
        "            # Create mask to exclude class c\n",
        "            mask = torch.ones_like(logits, dtype=torch.bool)\n",
        "            mask[:, c] = False\n",
        "            g_k, _ = torch.max(logits.masked_fill(~mask, -float('inf')), dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "            margin = g_c - g_k  # Shape: (batch_size,)\n",
        "\n",
        "            # Update max margins and triggers\n",
        "            update_mask = margin > max_margins_batch\n",
        "            max_margins_batch = torch.maximum(max_margins_batch, margin)\n",
        "            triggers_batch[update_mask] = optimized_imgs.detach()[update_mask]\n",
        "\n",
        "            # Loss is negative margin\n",
        "            loss = -margin.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Project images\n",
        "            with torch.no_grad():\n",
        "                optimized_imgs.copy_(project_image(optimized_imgs, projection_mean, projection_std))\n",
        "\n",
        "            # Check for convergence\n",
        "            relative_change = torch.abs(margin - f_old) / (torch.abs(f_old) + 1e-8)\n",
        "            if torch.all(relative_change < tolerance):\n",
        "                LOGGER.info(f\"  Converged at iteration {iteration} for class {c}.\")\n",
        "                break\n",
        "            f_old = margin.clone()\n",
        "\n",
        "        # Store results\n",
        "        max_margins[c] = max_margins_batch.max().item()\n",
        "        all_margins_per_class[c] = max_margins_batch.cpu().tolist()\n",
        "        all_triggers_per_class[c] = [triggers_batch[i].cpu() for i in range(batch_size)]\n",
        "        LOGGER.info(f\"  Maximum Margin for class {c}: {max_margins[c]:.4f}\")\n",
        "\n",
        "    # Sort max_margins by class\n",
        "    max_margins_sorted = [max_margins.get(c, 0.0) for c in range(num_classes)]\n",
        "\n",
        "    return max_margins_sorted, all_triggers_per_class, all_margins_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159514be-a20d-4fe1-adfb-f9b1ee0541bd",
      "metadata": {
        "id": "159514be-a20d-4fe1-adfb-f9b1ee0541bd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm  # For progress bars\n",
        "\n",
        "def compute_max_margin_parallel(\n",
        "    model: PreActResNet,\n",
        "    selected_images_per_class: Dict[int, List[torch.Tensor]],\n",
        "    num_classes: int,\n",
        "    projection_mean: float,\n",
        "    projection_std: float,\n",
        "    max_iterations: int = 1000,\n",
        "    lr: float = 0.01,\n",
        "    tolerance: float = 1e-5,\n",
        "    max_img_per_class: int = 3,\n",
        "    device: str = DEVICE,  # Ensure DEVICE is defined\n",
        "    parallel_classes: int = 15  # Number of classes to process in parallel\n",
        ") -> Tuple[List[float], Dict[int, List[torch.Tensor]], Dict[int, List[float]]]:\n",
        "\n",
        "    model.eval()\n",
        "    max_margins = {}\n",
        "    all_margins_per_class = defaultdict(list)\n",
        "    all_triggers_per_class = defaultdict(list)\n",
        "\n",
        "    # Prepare classes in batches for parallel processing\n",
        "    class_indices = list(range(num_classes))\n",
        "    class_batches = [\n",
        "        class_indices[i:i + parallel_classes]\n",
        "        for i in range(0, len(class_indices), parallel_classes)\n",
        "    ]\n",
        "\n",
        "    for batch_num, class_batch in enumerate(class_batches):\n",
        "        LOGGER.info(f\"\\nProcessing Batch {batch_num + 1}/{len(class_batches)}: Classes {class_batch}\")\n",
        "\n",
        "        # Gather all images to optimize across the current batch of classes\n",
        "        batch_images = []\n",
        "        batch_class_labels = []\n",
        "        class_to_image_indices = defaultdict(list)  # Mapping from class to image indices in batch_images\n",
        "\n",
        "        for c in class_batch:\n",
        "            images_to_optimize = []\n",
        "            for k in range(num_classes):\n",
        "                if k == c:\n",
        "                    continue\n",
        "                images = selected_images_per_class.get(k, [])\n",
        "                if not images:\n",
        "                    continue\n",
        "                images_to_optimize.extend(images)\n",
        "            random.shuffle(images_to_optimize)\n",
        "            images_to_optimize = images_to_optimize[:max_img_per_class]\n",
        "\n",
        "            if not images_to_optimize:\n",
        "                max_margins[c] = 0.0\n",
        "                LOGGER.info(f\"  No images to optimize for class {c}.\")\n",
        "                continue\n",
        "\n",
        "            LOGGER.info(f\"  Class {c}: {len(images_to_optimize)} images to optimize.\")\n",
        "\n",
        "            start_idx = len(batch_images)\n",
        "            batch_images.extend([img.clone().detach() for img in images_to_optimize])\n",
        "            batch_class_labels.extend([c] * len(images_to_optimize))\n",
        "            for i in range(len(images_to_optimize)):\n",
        "                class_to_image_indices[c].append(start_idx + i)\n",
        "\n",
        "        if not batch_images:\n",
        "            LOGGER.info(\"  No images to optimize in this batch.\")\n",
        "            continue\n",
        "\n",
        "        # Create a batch tensor of images\n",
        "        optimized_imgs = torch.stack(batch_images).to(device)\n",
        "        optimized_imgs.requires_grad = True\n",
        "\n",
        "        # Initialize optimizer for all images in the batch\n",
        "        optimizer = Adam([optimized_imgs], lr=lr) if lr is not None else Adam([optimized_imgs])\n",
        "\n",
        "        # Initialize tracking tensors\n",
        "        batch_size = optimized_imgs.size(0)\n",
        "        max_margins_batch = torch.full((batch_size,), -float('inf'), device=device)\n",
        "        triggers_batch = optimized_imgs.clone().detach()\n",
        "        f_old = torch.full((batch_size,), float('inf'), device=device)\n",
        "\n",
        "        # Optimization loop\n",
        "        for iteration in tqdm(range(max_iterations)):\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            probs, logits = get_logits_and_probs(model, optimized_imgs)  # Assume batch processing\n",
        "            # logits shape: (batch_size, num_classes)\n",
        "\n",
        "            # Gather target class indices for each image\n",
        "            target_classes = torch.tensor(batch_class_labels, device=device)\n",
        "            g_c = logits[torch.arange(batch_size), target_classes]  # Shape: (batch_size,)\n",
        "\n",
        "            # Create mask to exclude target classes\n",
        "            mask = torch.ones_like(logits, dtype=torch.bool)\n",
        "            mask[torch.arange(batch_size), target_classes] = False\n",
        "            # Set excluded logits to -inf for max computation\n",
        "            g_k, _ = torch.max(logits.masked_fill(~mask, -float('inf')), dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "            margin = g_c - g_k  # Shape: (batch_size,)\n",
        "\n",
        "            # Update max margins and triggers\n",
        "            update_mask = margin > max_margins_batch\n",
        "            max_margins_batch = torch.maximum(max_margins_batch, margin)\n",
        "            triggers_batch[update_mask] = optimized_imgs.detach()[update_mask]\n",
        "\n",
        "            # Compute loss as the negative margin\n",
        "            loss = -margin.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Project images\n",
        "            with torch.no_grad():\n",
        "                optimized_imgs.copy_(\n",
        "                   project_image(optimized_imgs, projection_mean, projection_std)\n",
        "                    )\n",
        "\n",
        "            # Check for convergence\n",
        "            relative_change = torch.abs(margin - f_old) / (torch.abs(f_old) + 1e-8)\n",
        "            if torch.all(relative_change < tolerance):\n",
        "                LOGGER.info(f\"  Converged at iteration {iteration} for batch {batch_num + 1}.\")\n",
        "                break\n",
        "            f_old = margin.clone()\n",
        "\n",
        "        # Store results per class\n",
        "        for c in class_batch:\n",
        "            image_indices = class_to_image_indices.get(c, [])\n",
        "            if not image_indices:\n",
        "                continue\n",
        "            class_margins = max_margins_batch[image_indices].cpu().tolist()\n",
        "            class_triggers = triggers_batch[image_indices].detach().cpu()\n",
        "\n",
        "            all_margins_per_class[c].extend(class_margins)\n",
        "            all_triggers_per_class[c].extend(class_triggers)\n",
        "\n",
        "            max_margins[c] = max(class_margins)\n",
        "            LOGGER.info(f\"  Maximum Margin for class {c}: {max_margins[c]:.4f}\")\n",
        "\n",
        "    # Sort max_margins by class\n",
        "    max_margins_sorted = [max_margins.get(c, 0.0) for c in range(num_classes)]\n",
        "\n",
        "    return max_margins_sorted, all_triggers_per_class, all_margins_per_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K0ykYeHU34P0",
      "metadata": {
        "id": "K0ykYeHU34P0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm  # Optional: For progress bars\n",
        "\n",
        "def compute_max_margin_parallel_with_epsilon(\n",
        "    model: PreActResNet,\n",
        "    selected_images_per_class: Dict[int, List[torch.Tensor]],\n",
        "    reference_images_per_class: Dict[int, List[torch.Tensor]],\n",
        "    num_classes: int,\n",
        "    projection_mean: float,\n",
        "    projection_std: float,\n",
        "    epsilon: float,\n",
        "    max_iterations: int = 1000,\n",
        "    lr: float = 0.01,\n",
        "    tolerance: float = 1e-5,\n",
        "    max_img_per_class: int = 3,\n",
        "    device: str= DEVICE,  # Ensure DEVICE is defined\n",
        "    parallel_classes: int = 50  # Number of classes to process in parallel\n",
        ") -> Tuple[List[float], Dict[int, List[torch.Tensor]], Dict[int, List[float]]]:\n",
        "    \"\"\"\n",
        "    Computes the maximum margin for each class by optimizing multiple classes in parallel on the GPU,\n",
        "    ensuring that optimized images stay within epsilon distance from their reference images.\n",
        "\n",
        "    Args:\n",
        "        model (PreActResNet): The neural network model.\n",
        "        selected_images_per_class (Dict[int, List[torch.Tensor]]): Dictionary mapping class indices to lists of images to optimize.\n",
        "        reference_images_per_class (Dict[int, List[torch.Tensor]]): Dictionary mapping class indices to lists of reference images.\n",
        "        num_classes (int): Total number of classes.\n",
        "        projection_mean (float): Mean for projection normalization.\n",
        "        projection_std (float): Standard deviation for projection normalization.\n",
        "        epsilon (float): Maximum allowable distance from the reference image.\n",
        "        max_iterations (int, optional): Maximum number of optimization iterations. Defaults to 1000.\n",
        "        lr (float, optional): Learning rate for the optimizer. Defaults to 0.01.\n",
        "        tolerance (float, optional): Tolerance for convergence. Defaults to 1e-5.\n",
        "        max_img_per_class (int, optional): Maximum number of images to optimize per class. Defaults to 3.\n",
        "        device (torch.device, optional): The device to perform computations on. Defaults to DEVICE.\n",
        "        parallel_classes (int, optional): Number of classes to process in parallel. Defaults to 8.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], Dict[int, List[torch.Tensor]], Dict[int, List[float]]]:\n",
        "            - List of maximum margins sorted by class.\n",
        "            - Dictionary mapping class indices to lists of trigger images.\n",
        "            - Dictionary mapping class indices to lists of margins.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    max_margins = {}\n",
        "    all_margins_per_class = defaultdict(list)\n",
        "    all_triggers_per_class = defaultdict(list)\n",
        "\n",
        "    # Prepare classes in batches for parallel processing\n",
        "    class_indices = list(range(num_classes))\n",
        "    class_batches = [\n",
        "        class_indices[i:i + parallel_classes]\n",
        "        for i in range(0, len(class_indices), parallel_classes)\n",
        "    ]\n",
        "\n",
        "    for batch_num, class_batch in enumerate(class_batches):\n",
        "        LOGGER.info(f\"\\nProcessing Batch {batch_num + 1}/{len(class_batches)}: Classes {class_batch}\")\n",
        "\n",
        "        # Gather all images and their reference images to optimize across the current batch of classes\n",
        "        batch_images = []\n",
        "        batch_reference_images = []\n",
        "        batch_class_labels = []\n",
        "        class_to_image_indices = defaultdict(list)  # Mapping from class to image indices in batch_images\n",
        "\n",
        "        for c in class_batch:\n",
        "            images_to_optimize = []\n",
        "            reference_images = []\n",
        "            for k in range(num_classes):\n",
        "                if k == c:\n",
        "                    continue\n",
        "                images = selected_images_per_class.get(k, [])\n",
        "                references = reference_images_per_class.get(k, [])\n",
        "                if not images:\n",
        "                    continue\n",
        "                # Ensure the number of reference images matches\n",
        "                if len(references) < len(images):\n",
        "                    raise ValueError(f\"Not enough reference images for class {k}.\")\n",
        "                images_to_optimize.extend(images)\n",
        "                reference_images.extend(references[:len(images)])\n",
        "            random.shuffle(images_to_optimize)\n",
        "            images_to_optimize = images_to_optimize[:max_img_per_class]\n",
        "            reference_images = reference_images[:max_img_per_class]\n",
        "\n",
        "            if not images_to_optimize:\n",
        "                max_margins[c] = 0.0\n",
        "                LOGGER.info(f\"  No images to optimize for class {c}.\")\n",
        "                continue\n",
        "\n",
        "            LOGGER.info(f\"  Class {c}: {len(images_to_optimize)} images to optimize.\")\n",
        "\n",
        "            start_idx = len(batch_images)\n",
        "            batch_images.extend([img.clone().detach() for img in images_to_optimize])\n",
        "            batch_reference_images.extend([ref.clone().detach() for ref in reference_images])\n",
        "            batch_class_labels.extend([c] * len(images_to_optimize))\n",
        "            for i in range(len(images_to_optimize)):\n",
        "                class_to_image_indices[c].append(start_idx + i)\n",
        "\n",
        "        if not batch_images:\n",
        "            LOGGER.info(\"  No images to optimize in this batch.\")\n",
        "            continue\n",
        "\n",
        "        # Create a batch tensor of images and reference images\n",
        "        optimized_imgs = torch.stack(batch_images).to(device)\n",
        "        reference_imgs = torch.stack(batch_reference_images).to(device)\n",
        "        optimized_imgs.requires_grad = True\n",
        "\n",
        "        # Initialize optimizer for all images in the batch\n",
        "        optimizer = Adam([optimized_imgs], lr=lr) if lr is not None else Adam([optimized_imgs])\n",
        "\n",
        "        # Initialize tracking tensors\n",
        "        batch_size = optimized_imgs.size(0)\n",
        "        max_margins_batch = torch.full((batch_size,), -float('inf'), device=device)\n",
        "        triggers_batch = optimized_imgs.clone().detach()\n",
        "        f_old = torch.full((batch_size,), float('inf'), device=device)\n",
        "\n",
        "        # Optimization loop\n",
        "        for iteration in tqdm(range(max_iterations)):\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            probs, logits = get_logits_and_probs(model, optimized_imgs)  # Assume batch processing\n",
        "            # logits shape: (batch_size, num_classes)\n",
        "\n",
        "            # Gather target class indices for each image\n",
        "            target_classes = torch.tensor(batch_class_labels, device=device)\n",
        "            g_c = logits[torch.arange(batch_size), target_classes]  # Shape: (batch_size,)\n",
        "\n",
        "            # Create mask to exclude target classes\n",
        "            mask = torch.ones_like(logits, dtype=torch.bool)\n",
        "            mask[torch.arange(batch_size), target_classes] = False\n",
        "            # Set excluded logits to -inf for max computation\n",
        "            g_k, _ = torch.max(logits.masked_fill(~mask, -float('inf')), dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "            margin = g_c - g_k  # Shape: (batch_size,)\n",
        "\n",
        "            # Update max margins and triggers\n",
        "            update_mask = margin > max_margins_batch\n",
        "            max_margins_batch = torch.maximum(max_margins_batch, margin)\n",
        "            triggers_batch[update_mask] = optimized_imgs.detach()[update_mask]\n",
        "\n",
        "            # Compute loss as the negative margin\n",
        "            loss = -margin.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Project images with epsilon constraint\n",
        "            with torch.no_grad():\n",
        "                optimized_imgs.copy_(project_image_with_epsilon(\n",
        "                    optimized_imgs,\n",
        "                    reference_imgs,\n",
        "                    projection_mean,\n",
        "                    projection_std,\n",
        "                    epsilon\n",
        "                ))\n",
        "\n",
        "            # Check for convergence\n",
        "            relative_change = torch.abs(margin - f_old) / (torch.abs(f_old) + 1e-8)\n",
        "            if torch.all(relative_change < tolerance):\n",
        "                #LOGGER.info(f\"  Converged at iteration {iteration + 1} for batch {batch_num + 1}.\")\n",
        "                break\n",
        "            f_old = margin.clone()\n",
        "\n",
        "            # Optional: Log progress every 100 iterations\n",
        "            if (iteration + 1) % 100 == 0:\n",
        "                pass\n",
        "                #LOGGER.info(f\"  Iteration {iteration + 1}/{max_iterations} for batch {batch_num + 1}.\")\n",
        "\n",
        "        # Store results per class\n",
        "        for c in class_batch:\n",
        "            image_indices = class_to_image_indices.get(c, [])\n",
        "            if not image_indices:\n",
        "                continue\n",
        "            class_margins = max_margins_batch[image_indices].cpu().tolist()\n",
        "            class_triggers = triggers_batch[image_indices].detach().cpu()\n",
        "\n",
        "            all_margins_per_class[c].extend(class_margins)\n",
        "            all_triggers_per_class[c].extend(class_triggers)\n",
        "\n",
        "            max_margins[c] = max(class_margins)\n",
        "            LOGGER.info(f\"  Maximum Margin for class {c}: {max_margins[c]:.4f}\")\n",
        "\n",
        "    # Sort max_margins by class\n",
        "    max_margins_sorted = [max_margins.get(c, 0.0) for c in range(num_classes)]\n",
        "\n",
        "    return max_margins_sorted, all_triggers_per_class, all_margins_per_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e72d3ffd-837f-416d-ae3c-b06a7d7ecbf1",
      "metadata": {
        "id": "e72d3ffd-837f-416d-ae3c-b06a7d7ecbf1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def compute_p_values(gamma_list: List[float],\n",
        "                     distributions=['gamma', 'norm'], p_value_type='standard') -> dict[str, float]:\n",
        "\n",
        "    if not gamma_list:\n",
        "        raise ValueError(\"gamma_list is empty.\")\n",
        "\n",
        "    gamma_array = np.array(gamma_list)\n",
        "    r_max = np.max(gamma_array)\n",
        "    n = len(gamma_array)\n",
        "\n",
        "    null_data = gamma_array[gamma_array != r_max]\n",
        "    if len(null_data) == 0:\n",
        "        raise ValueError(\"All values in gamma_list are identical.\")\n",
        "\n",
        "    p_values = {}\n",
        "\n",
        "    for dist_name in distributions:\n",
        "        try:\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "\n",
        "                if dist_name == 'gamma':\n",
        "                    a, loc, scale = stats.gamma.fit(null_data, floc=0)\n",
        "                    fitted_dist = stats.gamma(a, loc=loc, scale=scale)\n",
        "                elif dist_name == 'norm':\n",
        "                    mu, sigma = stats.norm.fit(null_data)\n",
        "                    fitted_dist = stats.norm(loc=mu, scale=sigma)\n",
        "                elif dist_name == 'expon':\n",
        "                    loc, scale = stats.expon.fit(null_data)\n",
        "                    fitted_dist = stats.expon(loc=loc, scale=scale)\n",
        "                elif dist_name == 'beta':\n",
        "                    a, b, loc, scale = stats.beta.fit(null_data, floc=0, fscale=1)\n",
        "                    fitted_dist = stats.beta(a, b, loc=loc, scale=scale)\n",
        "                elif dist_name == 'lognorm':\n",
        "                    s, loc, scale = stats.lognorm.fit(null_data, floc=0)\n",
        "                    fitted_dist = stats.lognorm(s, loc=loc, scale=scale)\n",
        "                else:\n",
        "                    LOGGER.error(f\"Distribution '{dist_name}' is not supported.\")\n",
        "                    continue\n",
        "\n",
        "                H0_r_max = fitted_dist.cdf(r_max)\n",
        "\n",
        "                if p_value_type == 'standard':\n",
        "                    p_val = 1 - H0_r_max**n\n",
        "                elif p_value_type == 'user_specified':\n",
        "                    p_val = H0_r_max**(n-1)\n",
        "                else:\n",
        "                    LOGGER.error(f\"p_value_type '{p_value_type}' is not recognized. Choose 'standard' or 'user_specified'.\")\n",
        "                    continue\n",
        "\n",
        "                p_values[dist_name] = p_val\n",
        "\n",
        "        except Exception as e:\n",
        "            LOGGER.error(f\"An error occurred while fitting distribution '{dist_name}': {e}\")\n",
        "            p_values[dist_name] = None\n",
        "\n",
        "    return p_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720b97aa-f272-4f04-96db-d4f3ca8adf97",
      "metadata": {
        "id": "720b97aa-f272-4f04-96db-d4f3ca8adf97"
      },
      "outputs": [],
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model.eval()\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.hook_handles.append(\n",
        "            self.target_layer.register_forward_hook(forward_hook)\n",
        "        )\n",
        "        self.hook_handles.append(\n",
        "            self.target_layer.register_backward_hook(backward_hook)\n",
        "        )\n",
        "\n",
        "    def generate_heatmap(self, input_tensor, class_idx=None):\n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax(dim=1).item()\n",
        "\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        target = output[0, class_idx]\n",
        "        target.backward()\n",
        "\n",
        "        # Compute weights\n",
        "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
        "\n",
        "        # Weight the activations\n",
        "        for i in range(self.activations.shape[1]):\n",
        "            self.activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "        # Compute the heatmap\n",
        "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
        "        heatmap = F.relu(heatmap)\n",
        "        heatmap /= torch.max(heatmap)\n",
        "\n",
        "        return heatmap.cpu().numpy()\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "def get_heatmap(model, inp):\n",
        "    last_child = None\n",
        "    for child in model.layer2.children():\n",
        "        last_child = child\n",
        "    target_layer = list(last_child.children())[3]\n",
        "    #target_layer = model.layer4[-1].conv3\n",
        "\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "\n",
        "\n",
        "    input_tensor = inp.unsqueeze(0).to(DEVICE)\n",
        "    # Generate heatmap\n",
        "    heatmap = grad_cam.generate_heatmap(input_tensor)\n",
        "    grad_cam.remove_hooks()\n",
        "    return heatmap\n",
        "\n",
        "def plot_heatmap(heatmap):\n",
        "    plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ca7c11-6684-42a5-8e63-97b8ac87affe",
      "metadata": {
        "id": "67ca7c11-6684-42a5-8e63-97b8ac87affe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(tensor, mean, std, title=None, save_dir=None, filename=None):\n",
        "    plt.figure()\n",
        "    img = tensor.clone()\n",
        "    if img.dim() == 4:\n",
        "        img = img[0]\n",
        "    num_channels = img.size(0)\n",
        "    if isinstance(mean, (list, tuple)):\n",
        "        if len(mean) == 1:\n",
        "            mean = [mean[0]] * num_channels\n",
        "        elif len(mean) != num_channels:\n",
        "            raise ValueError(f\"Length of mean ({len(mean)}) does not match number of channels ({num_channels}).\")\n",
        "        mean = torch.tensor(mean).to(DEVICE)\n",
        "    else:\n",
        "        mean = torch.tensor([mean] * num_channels).to(DEVICE)\n",
        "    if isinstance(std, (list, tuple)):\n",
        "        if len(std) == 1:\n",
        "            std = [std[0]] * num_channels\n",
        "        elif len(std) != num_channels:\n",
        "            raise ValueError(f\"Length of std ({len(std)}) does not match number of channels ({num_channels}).\")\n",
        "        std = torch.tensor(std).to(DEVICE)\n",
        "    else:\n",
        "        std = torch.tensor([std] * num_channels).to(DEVICE)\n",
        "    img = img.to(DEVICE)\n",
        "    for c in range(num_channels):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "    np_img = img.cpu().numpy()\n",
        "    if num_channels == 1:\n",
        "        np_img = np_img.squeeze(0)\n",
        "        plt.imshow(np_img, cmap='gray')\n",
        "    else:\n",
        "        np_img = np.transpose(np_img, (1, 2, 0))\n",
        "        plt.imshow(np_img)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    if save_dir and filename:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, f'{filename}.png')\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        LOGGER.info(f\"Saved image plot to {save_path}\")\n",
        "    plt.show()\n",
        "    LOGGER.info(f\"Plotted image: {title}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db707a1e-f4f6-4622-a1b6-b2216a36853c",
      "metadata": {
        "id": "db707a1e-f4f6-4622-a1b6-b2216a36853c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def select_neurons_and_get_activations_per_each(model, percent_per_layer, inputs: List[torch.Tensor]):\n",
        "    LOGGER.info(\"Selecting neurons and collecting activations\")\n",
        "    activation = defaultdict(list)\n",
        "    hooks = []\n",
        "    selected_neurons = {}\n",
        "    layers = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            layers.append(name)\n",
        "    for name in layers:\n",
        "        def get_activation(name):\n",
        "            def hook(model, input, output):\n",
        "                act = output.detach()\n",
        "                if act.dim() > 2:\n",
        "                    act = act.mean(dim=[2,3])\n",
        "                act = act.squeeze(0).cpu().numpy()\n",
        "                if act.ndim == 1:\n",
        "                    activation[name].append(act)\n",
        "                else:\n",
        "                    activation[name].append(act.flatten())\n",
        "            return hook\n",
        "        hooks.append(model.get_submodule(name).register_forward_hook(get_activation(name)))\n",
        "    LOGGER.info(\"Registered hooks\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img in tqdm(inputs, desc=\"Processing inputs\"):\n",
        "            model(img.unsqueeze(0))\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "    LOGGER.info(\"Collected activations\")\n",
        "    for name in layers:\n",
        "        acts = activation[name]\n",
        "        if len(acts) != len(inputs):\n",
        "            LOGGER.error(f\"Activation length mismatch for layer {name}: expected {len(inputs)}, got {len(acts)}\")\n",
        "            raise ValueError(f\"Activation length mismatch for layer {name}\")\n",
        "        act = np.stack(acts, axis=0)\n",
        "        num_neurons = act.shape[1]\n",
        "        selected = np.random.choice(num_neurons, max(1, int(num_neurons * percent_per_layer)), replace=False)\n",
        "        selected_neurons[name] = selected\n",
        "    LOGGER.info(\"Selected neurons per layer\")\n",
        "    activations_per_input = []\n",
        "    for img_idx in tqdm(range(len(inputs)), desc=\"Aggregating activations\"):\n",
        "        img_activations = []\n",
        "        for name in layers:\n",
        "            act = activation[name][img_idx]\n",
        "            selected = selected_neurons[name]\n",
        "            img_activations.extend(act[selected])\n",
        "        activations_per_input.append(img_activations)\n",
        "    LOGGER.info(\"Aggregated activations per input\")\n",
        "    return activations_per_input\n",
        "\n",
        "def compute_neuron_pairwise_correlation(activations: List[List[float]]) -> np.ndarray:\n",
        "    LOGGER.info(\"Computing neuron pairwise correlation\")\n",
        "    activation_matrix = np.array(activations)  # Shape: (num_inputs, num_neurons)\n",
        "    if activation_matrix.ndim != 2:\n",
        "        LOGGER.error(\"Activation matrix is not 2D\")\n",
        "        raise ValueError(\"Activation matrix must be 2D\")\n",
        "    corr_matrix = np.corrcoef(activation_matrix, rowvar=False)  # Correlation between neurons\n",
        "    LOGGER.info(\"Computed neuron pairwise correlation\")\n",
        "    return corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ff83ac-4dc1-4da0-916c-49d5c54925c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ff83ac-4dc1-4da0-916c-49d5c54925c3",
        "outputId": "d4c7bb17-bee1-4e1c-adde-0d5fef384de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-15 14:39:27--  https://huggingface.co/datasets/abbasfar/backdoor_attack_evaluation_dataset/resolve/main/eval_dataset.zip\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.61, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/44/d3/44d3f2884b0ce4544d589087f4734c7d7d5713c4209ec3c9f040c4bd2c393c1e/951aa8d3ee66c892f3c2cb660c7af40a74c274ece5d6d71cbe93385f8d7e0783?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27eval_dataset.zip%3B+filename%3D%22eval_dataset.zip%22%3B&response-content-type=application%2Fzip&Expires=1734532767&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDUzMjc2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQ0L2QzLzQ0ZDNmMjg4NGIwY2U0NTQ0ZDU4OTA4N2Y0NzM0YzdkN2Q1NzEzYzQyMDllYzNjOWYwNDBjNGJkMmMzOTNjMWUvOTUxYWE4ZDNlZTY2Yzg5MmYzYzJjYjY2MGM3YWY0MGE3NGMyNzRlY2U1ZDZkNzFjYmU5MzM4NWY4ZDdlMDc4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=kxC7X%7EQ1ndslYyWPAu5OP3RlS9ZViW%7EvpaMK033nWbW%7Edqmvqc7trlWWr-j-14RJtvwE%7E-ycwXPSY0in1OoNwcFrVtlu03uWUWuD5jP-Q%7EpHvzhlM2QchszxsuGNqQxQZaAnV8LF8P4-KSOtJHwsxdnyVE87lBtECCBIYp9LRJrfJ-HN-ZV38yaR52eX2g7NGlKkfqe68mlyAlLoYGbkZlc50htWmvqV3sfolGWNMsNNRX9Ib001nwOqfC%7E3GajzHLSM6zFAzKP2Fog8nT0n91cnmiP44hfTWSFUy3sSs-74PPE8QN%7EQuhAQ15MkZ4Amw9K1cUw0VMDNPAebH4WGhQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-12-15 14:39:27--  https://cdn-lfs-us-1.hf.co/repos/44/d3/44d3f2884b0ce4544d589087f4734c7d7d5713c4209ec3c9f040c4bd2c393c1e/951aa8d3ee66c892f3c2cb660c7af40a74c274ece5d6d71cbe93385f8d7e0783?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27eval_dataset.zip%3B+filename%3D%22eval_dataset.zip%22%3B&response-content-type=application%2Fzip&Expires=1734532767&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDUzMjc2N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQ0L2QzLzQ0ZDNmMjg4NGIwY2U0NTQ0ZDU4OTA4N2Y0NzM0YzdkN2Q1NzEzYzQyMDllYzNjOWYwNDBjNGJkMmMzOTNjMWUvOTUxYWE4ZDNlZTY2Yzg5MmYzYzJjYjY2MGM3YWY0MGE3NGMyNzRlY2U1ZDZkNzFjYmU5MzM4NWY4ZDdlMDc4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=kxC7X%7EQ1ndslYyWPAu5OP3RlS9ZViW%7EvpaMK033nWbW%7Edqmvqc7trlWWr-j-14RJtvwE%7E-ycwXPSY0in1OoNwcFrVtlu03uWUWuD5jP-Q%7EpHvzhlM2QchszxsuGNqQxQZaAnV8LF8P4-KSOtJHwsxdnyVE87lBtECCBIYp9LRJrfJ-HN-ZV38yaR52eX2g7NGlKkfqe68mlyAlLoYGbkZlc50htWmvqV3sfolGWNMsNNRX9Ib001nwOqfC%7E3GajzHLSM6zFAzKP2Fog8nT0n91cnmiP44hfTWSFUy3sSs-74PPE8QN%7EQuhAQ15MkZ4Amw9K1cUw0VMDNPAebH4WGhQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.160.3, 3.165.160.20, 3.165.160.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.160.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1610284884 (1.5G) [application/zip]\n",
            "Saving to: ‘eval_dataset.zip’\n",
            "\n",
            "eval_dataset.zip      4%[                    ]  65.14M  40.2MB/s               ^C\n"
          ]
        }
      ],
      "source": [
        "! wget https://huggingface.co/datasets/abbasfar/backdoor_attack_evaluation_dataset/resolve/main/eval_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wavcH5KQtAzN",
      "metadata": {
        "id": "wavcH5KQtAzN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf62cc91-68a1-416f-944a-9e55a42aa885",
      "metadata": {
        "id": "cf62cc91-68a1-416f-944a-9e55a42aa885"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from statistics import mean\n",
        "\n",
        "\n",
        "def analyze(idx):\n",
        "    model, num_classes, ground_truth, transformation, images_root_dir = load_test(idx)\n",
        "    LOGGER.info(f'\\n\\n\\nmodel id: {idx} {\"is malicous\" if ground_truth else \"is clean\"} with {num_classes} classes\\n\\n\\n')\n",
        "\n",
        "    t = time.time()\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    images_path = glob.glob(os.path.join(images_root_dir, '*.jpg'))\n",
        "    labels = [int(image_path.split('_')[-1].split('.')[0]) for image_path in images_path]\n",
        "\n",
        "    projection_mean, projection_std = extract_normalization_params(transformation)\n",
        "    transformed_images = transform_images(images_path, transformation)\n",
        "\n",
        "    probs, logits = get_logits_and_probs(model, transformed_images)\n",
        "    accepted_margins, failed_margins = calculate_margins(logits, labels)\n",
        "    safe_margins = [x[1] for x in\n",
        "                    sorted(find_safe_margin(accepted_margins, failed_margins).items(),\n",
        "                           key=lambda x: x[0])]\n",
        "\n",
        "\n",
        "    k = 9\n",
        "    max_iterations = int(1000 * (10 / num_classes))\n",
        "    max_img_per_class =  max(int(9 * (10 / num_classes)), 1)\n",
        "\n",
        "\n",
        "    epsilon = 3\n",
        "    confident_images_per_class = select_top_images_per_class(probs, transformed_images,\n",
        "                                    labels, num_classes, top_k=9)\n",
        "    lr = None\n",
        "    tolerance = 1e-5\n",
        "\n",
        "    if max_img_per_class <= 2:\n",
        "      confident_images_per_class = {c: [generate_random_image(transformation) for i in range(k)]\n",
        "                                   for c in range(num_classes)}\n",
        "      epsilon = 100\n",
        "      lr = 0.1\n",
        "      tolerance = 1e-3\n",
        "\n",
        "    max_margins, triggers_per_class, all_margins_per_class = compute_max_margin_parallel_with_epsilon(model,\n",
        "                                      confident_images_per_class, confident_images_per_class,\n",
        "                                       num_classes, projection_mean, projection_std,\n",
        "                                       max_iterations=max_iterations,\n",
        "                                       lr=lr,\n",
        "                                       tolerance=tolerance,\n",
        "                                       epsilon = epsilon,\n",
        "                                       max_img_per_class=max_img_per_class)\n",
        "\n",
        "    for i in range(len(max_margins)):\n",
        "      max_margins[i] = max(max_margins[i], 0)\n",
        "    average_margins = list()\n",
        "\n",
        "    for c in range(num_classes):\n",
        "      for i in range(len(all_margins_per_class[c])):\n",
        "        all_margins_per_class[c][i] = max(all_margins_per_class[c][i], 0)\n",
        "      average_margins.append(mean(all_margins_per_class[c]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    LOGGER.info(f'finding adversaries has taken {round(time.time() - t, 2)} seconds')\n",
        "    for i in range(num_classes):\n",
        "        target_class_c = max_margins.index(max(max_margins))\n",
        "        LOGGER.info(f'working on class {target_class_c}')\n",
        "        p_values_standard = compute_p_values(max_margins, distributions=['gamma', 'norm', 'expon'],\n",
        "                                             p_value_type='standard')\n",
        "        avg_p_values_standard = compute_p_values(average_margins, distributions=['gamma', 'norm', 'expon'],\n",
        "                                             p_value_type='standard')\n",
        "        safe_p_values_standard = compute_p_values(safe_margins, distributions=['gamma', 'norm', 'expon'],\n",
        "                                                p_value_type='standard')\n",
        "        LOGGER.info(f'max margin p values {p_values_standard}')\n",
        "        LOGGER.info(f'average maximum margins p_values {avg_p_values_standard}')\n",
        "        LOGGER.info(f'safe margins p values {safe_p_values_standard}')\n",
        "\n",
        "        if p_values_standard['gamma'] <= 0.05:\n",
        "            LOGGER.info(f'detected as a malicious model and it {\"is malicous\" if ground_truth else \"is clean\"}')\n",
        "        else:\n",
        "            LOGGER.info(f'detected as a clean model and it {\"is malicous\" if ground_truth else \"is clean\"}')\n",
        "        break\n",
        "        LOGGER.info('plotting confident test imags')\n",
        "        for i in range(len(confident_images_per_class[target_class_c])):\n",
        "            img = confident_images_per_class[target_class_c][i]\n",
        "            imshow(img, projection_mean, projection_std, title=f'clean from class {target_class_c} num {i}')\n",
        "            heatmap = get_heatmap(model, img)\n",
        "            plot_heatmap(heatmap)\n",
        "\n",
        "        LOGGER.info('plotting adversary imags')\n",
        "        for i in range(len(triggers_per_class[target_class_c])):\n",
        "            trigger = triggers_per_class[target_class_c][i]\n",
        "            #LOGGER.info(f'adversary max margin is {}')\n",
        "            imshow(trigger, projection_mean, projection_std, title=f'adversary from class {target_class_c} num {i} with margin {all_margins_per_class[target_class_c][i]}')\n",
        "            heatmap = get_heatmap(model, trigger)\n",
        "            plot_heatmap(heatmap)\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d0a2c0-90d8-46d6-a90e-2c5696b54f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "59d0a2c0-90d8-46d6-a90e-2c5696b54f76",
        "outputId": "b0d1f50d-b5d0-4b0b-e208-74a87974fbf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "model id: 0 is malicous with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f16642e65afb>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "<ipython-input-3-f16642e65afb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "INFO:detector:\n",
            "\n",
            "\n",
            "model id: 0 is malicous with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 9: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 9: 9 images to optimize.\n",
            "100%|██████████| 1000/1000 [01:07<00:00, 14.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 0: 34.0931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:detector:  Maximum Margin for class 0: 34.0931\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 1: 23.3818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 1: 23.3818\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 2: 39.0376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 2: 39.0376\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 3: 43.1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 3: 43.1500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 4: 42.8557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 4: 42.8557\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 5: 31.9943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 5: 31.9943\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 6: 41.5265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 6: 41.5265\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 7: 50.9616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 7: 50.9616\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 8: 60.9890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 8: 60.9890\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 9: 28.2808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 9: 28.2808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finding adversaries has taken 68.24 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:finding adversaries has taken 68.24 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on class 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:working on class 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max margin p values {'gamma': 0.06466686368661678, 'norm': 0.016222460737541522, 'expon': 0.4973051202143991}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:max margin p values {'gamma': 0.06466686368661678, 'norm': 0.016222460737541522, 'expon': 0.4973051202143991}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average maximum margins p_values {'gamma': 0.015278860248490012, 'norm': 0.0013928470569556373, 'expon': 0.352606338027749}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:average maximum margins p_values {'gamma': 0.015278860248490012, 'norm': 0.0013928470569556373, 'expon': 0.352606338027749}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "safe margins p values {'gamma': 0.5122724966100904, 'norm': 0.2758160092490499, 'expon': 0.7948359785275755}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:safe margins p values {'gamma': 0.5122724966100904, 'norm': 0.2758160092490499, 'expon': 0.7948359785275755}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "detected as a clean model and it is malicous\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:detected as a clean model and it is malicous\n",
            "<ipython-input-3-f16642e65afb>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "model id: 1 is malicous with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f16642e65afb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "INFO:detector:\n",
            "\n",
            "\n",
            "model id: 1 is malicous with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 9: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 9: 9 images to optimize.\n",
            "100%|██████████| 1000/1000 [01:07<00:00, 14.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 0: 37.8012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:detector:  Maximum Margin for class 0: 37.8012\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 1: 21.7733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 1: 21.7733\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 2: 59.8119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 2: 59.8119\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 3: 59.6252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 3: 59.6252\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 4: 42.5500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 4: 42.5500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 5: 29.4279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 5: 29.4279\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 6: 39.5594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 6: 39.5594\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 7: 48.5680\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 7: 48.5680\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 8: 108.1978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 8: 108.1978\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 9: 32.6390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 9: 32.6390\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finding adversaries has taken 68.23 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:finding adversaries has taken 68.23 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on class 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:working on class 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max margin p values {'gamma': 0.0005219856478486662, 'norm': 2.2378205144502061e-07, 'expon': 0.11353914971768908}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:max margin p values {'gamma': 0.0005219856478486662, 'norm': 2.2378205144502061e-07, 'expon': 0.11353914971768908}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average maximum margins p_values {'gamma': 4.2711373798964836e-05, 'norm': 1.571387464593954e-10, 'expon': 0.12961855491606888}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:average maximum margins p_values {'gamma': 4.2711373798964836e-05, 'norm': 1.571387464593954e-10, 'expon': 0.12961855491606888}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "safe margins p values {'gamma': 0.6636802870468987, 'norm': 0.5351735453435438, 'expon': 0.8164729401067057}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:safe margins p values {'gamma': 0.6636802870468987, 'norm': 0.5351735453435438, 'expon': 0.8164729401067057}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "detected as a malicious model and it is malicous\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:detected as a malicious model and it is malicous\n",
            "<ipython-input-3-f16642e65afb>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "model id: 2 is clean with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f16642e65afb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "INFO:detector:\n",
            "\n",
            "\n",
            "model id: 2 is clean with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 9: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 9: 9 images to optimize.\n",
            "100%|██████████| 1000/1000 [01:08<00:00, 14.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 0: 26.0299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:detector:  Maximum Margin for class 0: 26.0299\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 1: 34.7713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 1: 34.7713\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 2: 19.7186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 2: 19.7186\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 3: 27.0187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 3: 27.0187\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 4: 24.0070\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 4: 24.0070\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 5: 78.4388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 5: 78.4388\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 6: 21.7980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 6: 21.7980\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 7: 32.4394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 7: 32.4394\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 8: 38.1910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 8: 38.1910\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Maximum Margin for class 9: 29.2207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Maximum Margin for class 9: 29.2207\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finding adversaries has taken 68.49 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:finding adversaries has taken 68.49 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on class 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:working on class 5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max margin p values {'gamma': 5.141304049161022e-09, 'norm': 0.0, 'expon': 0.009275590900068575}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:max margin p values {'gamma': 5.141304049161022e-09, 'norm': 0.0, 'expon': 0.009275590900068575}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average maximum margins p_values {'gamma': 9.395163330339074e-05, 'norm': 3.928375336137435e-08, 'expon': 0.04507010512425069}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:average maximum margins p_values {'gamma': 9.395163330339074e-05, 'norm': 3.928375336137435e-08, 'expon': 0.04507010512425069}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "safe margins p values {'gamma': 0.32574485658904206, 'norm': 0.06399903348242941, 'expon': 0.14458735802543055}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:safe margins p values {'gamma': 0.32574485658904206, 'norm': 0.06399903348242941, 'expon': 0.14458735802543055}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "detected as a malicious model and it is clean\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:detected as a malicious model and it is clean\n",
            "<ipython-input-3-f16642e65afb>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "<ipython-input-3-f16642e65afb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "model id: 3 is clean with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:\n",
            "\n",
            "\n",
            "model id: 3 is clean with 10 classes\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 0: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 1: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 2: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 3: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 4: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 5: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 6: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 7: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 8: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Class 9: 9 images to optimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:detector:  Class 9: 9 images to optimize.\n",
            " 67%|██████▋   | 670/1000 [00:45<00:22, 14.82it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-db8c8a919cd5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-fab464e7e854>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mtolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     max_margins, triggers_per_class, all_margins_per_class = compute_max_margin_parallel_with_epsilon(model,\n\u001b[0m\u001b[1;32m     46\u001b[0m                                       \u001b[0mconfident_images_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfident_images_per_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                        \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-8d4d8bdc8d21>\u001b[0m in \u001b[0;36mcompute_max_margin_parallel_with_epsilon\u001b[0;34m(model, selected_images_per_class, reference_images_per_class, num_classes, projection_mean, projection_std, epsilon, max_iterations, lr, tolerance, max_img_per_class, device, parallel_classes)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Project images with epsilon constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 optimized_imgs.copy_(project_image_with_epsilon(\n\u001b[0m\u001b[1;32m    152\u001b[0m                     \u001b[0moptimized_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mreference_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8d8192f55f0f>\u001b[0m in \u001b[0;36mproject_image_with_epsilon\u001b[0;34m(optimized_img, reference_img, mean, std, epsilon)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Normalize the reference image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmean_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstd_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnormalized_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreference_img\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(50):\n",
        "    analyze(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7chjQmj6izy7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7chjQmj6izy7",
        "outputId": "5492d126-7a1d-4be1-bb9a-26be76cb4aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device is cuda\n",
            "/content/tester.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "/content/tester.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "eval_dataset/0/test_dataset\n",
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "  Class 0: 9 images to optimize.\n",
            "  Class 1: 9 images to optimize.\n",
            "  Class 2: 9 images to optimize.\n",
            "  Class 3: 9 images to optimize.\n",
            "  Class 4: 9 images to optimize.\n",
            "  Class 5: 9 images to optimize.\n",
            "  Class 6: 9 images to optimize.\n",
            "  Class 7: 9 images to optimize.\n",
            "  Class 8: 9 images to optimize.\n",
            "  Class 9: 9 images to optimize.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/tester.py\", line 49, in <module>\n",
            "    pred.append(backdoor_model_detector(model, num_classes, images_root_dir, transformation))\n",
            "  File \"/content/main.py\", line 73, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/content/main.py\", line 447, in backdoor_model_detector\n",
            "    max_margins, triggers_per_class, all_margins_per_class = compute_max_margin_parallel_with_epsilon(model,\n",
            "  File \"/content/main.py\", line 313, in compute_max_margin_parallel_with_epsilon\n",
            "    optimized_imgs.copy_(project_image_with_epsilon(\n",
            "  File \"/content/main.py\", line 166, in project_image_with_epsilon\n",
            "    mean_tensor = torch.tensor(mean).view(-1, 1, 1).to(optimized_img.device)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "! python tester.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b89c51-95d6-473b-bdab-8744e4658451",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b89c51-95d6-473b-bdab-8744e4658451",
        "outputId": "af577afe-84ea-43ef-f8e4-573917b5b275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.5G\n",
            "-rw-r--r-- 1 root root 1.5G Nov  1 05:13 eval_dataset.zip\n",
            "drwxr-xr-x 1 root root 4.0K Dec 12 14:22 sample_data\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 41\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 13\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 44\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 43\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 19\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 40\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 14\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 17\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 32\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 14:18 26\n"
          ]
        }
      ],
      "source": [
        "! ls -lrth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1vvqK72sBqq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1vvqK72sBqq",
        "outputId": "5bcbdfd0-78a7-455e-8b22-78cc72cd054f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-f16642e65afb>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "<ipython-input-3-f16642e65afb>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ],
      "source": [
        "for idx in range(50):\n",
        "  model, num_classes, ground_truth, transformation, images_root_dir = load_test(idx)\n",
        "  if num_classes != 10:\n",
        "    print(idx, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FdNwrIvaZrss",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdNwrIvaZrss",
        "outputId": "146e8e9a-f117-4c20-f132-b951f1bc006b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device is cuda\n",
            "/content/tester.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "/content/tester.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "eval_dataset/0/test_dataset\n",
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "  Class 0: 9 images to optimize.\n",
            "  Class 1: 9 images to optimize.\n",
            "  Class 2: 9 images to optimize.\n",
            "  Class 3: 9 images to optimize.\n",
            "  Class 4: 9 images to optimize.\n",
            "  Class 5: 9 images to optimize.\n",
            "  Class 6: 9 images to optimize.\n",
            "  Class 7: 9 images to optimize.\n",
            "  Class 8: 9 images to optimize.\n",
            "  Class 9: 9 images to optimize.\n",
            "  Maximum Margin for class 0: 31.3799\n",
            "  Maximum Margin for class 1: 23.2311\n",
            "  Maximum Margin for class 2: 32.5545\n",
            "  Maximum Margin for class 3: 41.6083\n",
            "  Maximum Margin for class 4: 47.4671\n",
            "  Maximum Margin for class 5: 35.0290\n",
            "  Maximum Margin for class 6: 41.7243\n",
            "  Maximum Margin for class 7: 50.1137\n",
            "  Maximum Margin for class 8: 62.0549\n",
            "  Maximum Margin for class 9: 30.8738\n",
            "finding adversaries has taken 68.97 seconds\n",
            "working on class 8\n",
            "max margin p values {'gamma': 0.05010142198754863, 'norm': 0.011893517882241689, 'expon': 0.4668721390809789}\n",
            "average maximum margins p_values {'gamma': 0.028224969536293276, 'norm': 0.00466167190615685, 'expon': 0.41401365323235195}\n",
            "safe margins p values {'gamma': 0.5122724966100904, 'norm': 0.2758160092490499, 'expon': 0.7948359785275755}\n",
            "detected as a malicious model\n",
            "backdoor_model_detector took 68.995597 seconds\n",
            "True True\n",
            "/content/tester.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "/content/tester.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "eval_dataset/1/test_dataset\n",
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "  Class 0: 9 images to optimize.\n",
            "  Class 1: 9 images to optimize.\n",
            "  Class 2: 9 images to optimize.\n",
            "  Class 3: 9 images to optimize.\n",
            "  Class 4: 9 images to optimize.\n",
            "  Class 5: 9 images to optimize.\n",
            "  Class 6: 9 images to optimize.\n",
            "  Class 7: 9 images to optimize.\n",
            "  Class 8: 9 images to optimize.\n",
            "  Class 9: 9 images to optimize.\n",
            "  Maximum Margin for class 0: 38.9700\n",
            "  Maximum Margin for class 1: 21.1871\n",
            "  Maximum Margin for class 2: 57.0627\n",
            "  Maximum Margin for class 3: 50.4984\n",
            "  Maximum Margin for class 4: 43.8269\n",
            "  Maximum Margin for class 5: 33.1399\n",
            "  Maximum Margin for class 6: 44.1671\n",
            "  Maximum Margin for class 7: 50.2450\n",
            "  Maximum Margin for class 8: 106.9304\n",
            "  Maximum Margin for class 9: 35.0117\n",
            "finding adversaries has taken 66.76 seconds\n",
            "working on class 8\n",
            "max margin p values {'gamma': 8.810055061214683e-05, 'norm': 8.566347631244753e-10, 'expon': 0.1393018767329005}\n",
            "average maximum margins p_values {'gamma': 6.0360226233902026e-06, 'norm': 5.289102489314246e-12, 'expon': 0.059828331450652916}\n",
            "safe margins p values {'gamma': 0.6636802870468987, 'norm': 0.5351735453435438, 'expon': 0.8164729401067057}\n",
            "detected as a malicious model\n",
            "backdoor_model_detector took 66.783598 seconds\n",
            "True True\n",
            "/content/tester.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  metadata = torch.load(os.path.join(test_root_dir, 'metadata.pt'))\n",
            "/content/tester.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "eval_dataset/2/test_dataset\n",
            "\n",
            "Processing Batch 1/1: Classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "  Class 0: 9 images to optimize.\n",
            "  Class 1: 9 images to optimize.\n",
            "  Class 2: 9 images to optimize.\n",
            "  Class 3: 9 images to optimize.\n",
            "  Class 4: 9 images to optimize.\n",
            "  Class 5: 9 images to optimize.\n",
            "  Class 6: 9 images to optimize.\n",
            "  Class 7: 9 images to optimize.\n",
            "  Class 8: 9 images to optimize.\n",
            "  Class 9: 9 images to optimize.\n"
          ]
        }
      ],
      "source": [
        "! python tester.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iBPVbuPIjOZi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBPVbuPIjOZi",
        "outputId": "b6064dee-189d-4da8-ecc0-1c51c5f69799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-cifar'...\n",
            "remote: Enumerating objects: 382, done.\u001b[K\n",
            "remote: Total 382 (delta 0), reused 0 (delta 0), pack-reused 382 (from 1)\u001b[K\n",
            "Receiving objects: 100% (382/382), 81.31 KiB | 3.13 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/kuangliu/pytorch-cifar.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fYf71tYdrM_V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYf71tYdrM_V",
        "outputId": "1bbd3e8b-2fcd-4f46-ca18-92c365bf6e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pytorch-cifar  sample_data\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RdxbO2RhrS3u",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdxbO2RhrS3u",
        "outputId": "ee0ebd78-41c7-4fed-df10-81e48cc7d311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            " [================================================================>]  Step: 4s267ms | Tot: 53m37s | Loss: nan | Acc: 10.126% (5063/50000) 391/391 \n",
            " [================================================================>]  Step: 1s617ms | Tot: 2m47s | Loss: nan | Acc: 10.000% (1000/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n"
          ]
        }
      ],
      "source": [
        "! cd pytorch-cifar ; python main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qgSZc7rLrW7g",
      "metadata": {
        "id": "qgSZc7rLrW7g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}